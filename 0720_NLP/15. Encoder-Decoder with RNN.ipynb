{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN을 이용한 인코더-디코더"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 01) 시퀀스-투-시퀀스(Sequence-to-Sequence, seq2seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 시퀀스-투-시퀀스(seq2seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "시퀀스-투-시퀀스의 개념을 이해하려면 아래 페이지를 정독하세요.<br> https://wikidocs.net/24996"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 글자 레벨 기계 번역기(Character-level Neural Machine Translation) 구현하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "본 예제는 케라스 개발자 프랑소아 숄레의 게시물이 원본입니다.<br>\n",
    "https://blog.keras.io/a-ten-minute-introduction-to-sequence-to-sequence-learning-in-keras.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "실제 성능이 좋은 기계 번역기를 구현하려면 정말 방대한 데이터가 필요하므로 여기서는 방금 배운 seq2seq를 실습해보는 수준에서 아주 간단한 기계 번역기를 구축해보도록 하겠습니다. 기계 번역기를 훈련시키기 위해서는 훈련 데이터로 병렬 코퍼스(parallel corpus)가 필요합니다. 병렬 코퍼스란, 두 개 이상의 언어가 병렬적으로 구성된 코퍼스를 의미합니다.\n",
    "\n",
    "다운로드 링크 : http://www.manythings.org/anki"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이번 실습에서는 프랑스-영어 병렬 코퍼스인 fra-eng.zip 파일을 사용할 겁니다. 위의 링크에서 해당 파일을 다운받으시면 됩니다. 해당 파일의 압축을 풀면 fra.txt라는 파일이 있는데 이 파일이 이번 실습에서 사용할 파일입니다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1) 병렬 코퍼스 데이터에 대한 이해와 전처리"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "우선 병렬 코퍼스 데이터에 대한 이해를 해보겠습니다. 병렬 데이터라고 하면 앞서 수행한 태깅 작업의 데이터를 생각할 수 있지만, 앞서 수행한 태깅 작업의 병렬 데이터와 seq2seq가 사용하는 병렬 데이터는 성격이 조금 다릅니다. 태깅 작업의 병렬 데이터는 쌍이 되는 모든 데이터가 길이가 같았지만 여기서는 쌍이 된다고 해서 길이가 같지않습니다.\n",
    "\n",
    "실제 번역기를 생각해보면 구글 번역기에 '나는 학생이다.'라는 토큰의 개수가 2인 문장을 넣었을 때 'I am a student.'라는 토큰의 개수가 4인 문장이 나오는 것과 같은 이치입니다. seq2seq는 기본적으로 입력 시퀀스와 출력 시퀀스의 길이가 다를 수 있다고 가정합니다. 지금은 기계 번역기가 예제지만 seq2seq의 또 다른 유명한 예제 중 하나인 챗봇을 만든다고 가정해보면, 대답의 길이가 질문의 길이와 항상 똑같아야 한다고하면 그 또한 이상합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "여기서 사용할 fra.txt 데이터는 왼쪽의 영어 문장과 오른쪽의 프랑스어 문장 사이에 탭으로 구분되는 구조가 하나의 샘플입니다. 그리고 이와 같은 형식의 약 16만개의 병렬 문장 샘플을 포함하고 있습니다. 해당 데이터를 읽고 전처리를 진행해보겠습니다. 앞으로의 코드에서 src는 source의 줄임말로 입력 문장을 나타내며, tar는 target의 줄임말로 번역하고자 하는 문장을 나타냅니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import urllib3\n",
    "import zipfile\n",
    "import shutil\n",
    "import os\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "http = urllib3.PoolManager()\n",
    "url = 'http://www.manythings.org/anki/fra-eng.zip'\n",
    "filename = 'fra-eng.zip'\n",
    "path = os.getcwd()\n",
    "zipfilename = os.path.join(path, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with http.request('GET', url, preload_content=False) as r, open(zipfilename, 'wb') as out_file:\n",
    "    shutil.copyfileobj(r, out_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with zipfile.ZipFile(zipfilename, 'r') as zip_ref:\n",
    "    zip_ref.extractall(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "178009"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines = pd.read_csv('fra.txt', names=['src', 'tar', 'lic'], sep='\\t')\n",
    "del lines['lic']\n",
    "len(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>src</th>\n",
       "      <th>tar</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>38526</th>\n",
       "      <td>Tom didn't last long.</td>\n",
       "      <td>Tom n'a pas tenu longtemps.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1369</th>\n",
       "      <td>I remember.</td>\n",
       "      <td>Je m'en souviens.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34491</th>\n",
       "      <td>Focus on the details.</td>\n",
       "      <td>Concentre-toi sur les détails.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24759</th>\n",
       "      <td>I want this guitar.</td>\n",
       "      <td>Je veux cette guitare.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4158</th>\n",
       "      <td>It's a plant.</td>\n",
       "      <td>Il s'agit d'une plante.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26400</th>\n",
       "      <td>They are exhausted.</td>\n",
       "      <td>Elles sont exténuées.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6571</th>\n",
       "      <td>That's simple.</td>\n",
       "      <td>C'est simple.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50520</th>\n",
       "      <td>She has never seen him.</td>\n",
       "      <td>Elle ne l'a jamais vu.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32188</th>\n",
       "      <td>This is my father's.</td>\n",
       "      <td>Ceci appartient à mon père.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4126</th>\n",
       "      <td>It amazed me.</td>\n",
       "      <td>Ça m'a épaté.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           src                             tar\n",
       "38526    Tom didn't last long.     Tom n'a pas tenu longtemps.\n",
       "1369               I remember.               Je m'en souviens.\n",
       "34491    Focus on the details.  Concentre-toi sur les détails.\n",
       "24759      I want this guitar.          Je veux cette guitare.\n",
       "4158             It's a plant.         Il s'agit d'une plante.\n",
       "26400      They are exhausted.           Elles sont exténuées.\n",
       "6571            That's simple.                   C'est simple.\n",
       "50520  She has never seen him.          Elle ne l'a jamais vu.\n",
       "32188     This is my father's.     Ceci appartient à mon père.\n",
       "4126             It amazed me.                   Ça m'a épaté."
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 6만개만 가지고 해보겠습니다.\n",
    "\n",
    "lines = lines.loc[:, 'src':'tar']\n",
    "lines = lines[0:60000] # 6만개만 저장\n",
    "lines.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위의 테이블은 랜덤으로 선택된 10개의 샘플을 보여줍니다. 번역 문장에 해당되는 프랑스어 데이터는 앞서 배웠듯이 시작을 의미하는 심볼 \\<sos>와 종료를 의미하는 심볼 \\<eos>을 넣어주어야 합니다. 여기서는 \\<sos>와 \\<eos> 대신 '\\t'를 시작 심볼, '\\n'을 종료 심볼로 간주하여 추가하고 다시 데이터를 출력해보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>src</th>\n",
       "      <th>tar</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2017</th>\n",
       "      <td>Do it again!</td>\n",
       "      <td>\\t Fais-le encore une fois ! \\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38773</th>\n",
       "      <td>Tom wouldn't say yes.</td>\n",
       "      <td>\\t Tom n'accepterait pas. \\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47788</th>\n",
       "      <td>He wears thick glasses.</td>\n",
       "      <td>\\t Il porte des verres épais. \\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32591</th>\n",
       "      <td>Turn the radio down.</td>\n",
       "      <td>\\t Baisse la radio. \\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7201</th>\n",
       "      <td>You're no fun.</td>\n",
       "      <td>\\t Vous n'êtes pas drôle. \\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59665</th>\n",
       "      <td>You have to do as I say.</td>\n",
       "      <td>\\t Tu dois le faire comme je te le dis. \\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1594</th>\n",
       "      <td>Let me pay.</td>\n",
       "      <td>\\t Laissez-moi payer. \\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47142</th>\n",
       "      <td>Do you think I'm right?</td>\n",
       "      <td>\\t Penses-tu que j'ai raison ? \\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59960</th>\n",
       "      <td>You're the one who quit.</td>\n",
       "      <td>\\t Tu es celui qui a démissionné. \\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26937</th>\n",
       "      <td>Tom was real happy.</td>\n",
       "      <td>\\t Tom était très heureux. \\n</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            src                                         tar\n",
       "2017               Do it again!             \\t Fais-le encore une fois ! \\n\n",
       "38773     Tom wouldn't say yes.                \\t Tom n'accepterait pas. \\n\n",
       "47788   He wears thick glasses.            \\t Il porte des verres épais. \\n\n",
       "32591      Turn the radio down.                      \\t Baisse la radio. \\n\n",
       "7201             You're no fun.                \\t Vous n'êtes pas drôle. \\n\n",
       "59665  You have to do as I say.  \\t Tu dois le faire comme je te le dis. \\n\n",
       "1594                Let me pay.                    \\t Laissez-moi payer. \\n\n",
       "47142   Do you think I'm right?           \\t Penses-tu que j'ai raison ? \\n\n",
       "59960  You're the one who quit.        \\t Tu es celui qui a démissionné. \\n\n",
       "26937       Tom was real happy.               \\t Tom était très heureux. \\n"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines.tar = lines.tar.apply(lambda x : '\\t '+ x + ' \\n')\n",
    "lines.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 글자 집합을 생성해보겠습니다. 단어 집합이 아니라 글자 집합이라고 하는 이유는 토큰 단위가 단어가 아니라 글자이기 때문입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_vocab = set()\n",
    "for line in lines.src: # 1줄씩 읽음.\n",
    "    for char in line: # 1글자씩 읽음\n",
    "        src_vocab.add(char)\n",
    "        \n",
    "tar_vocab = set()\n",
    "for line in lines.tar: # 1줄씩 읽음.\n",
    "    for char in line: # 1글자씩 읽음\n",
    "        tar_vocab.add(char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79\n",
      "106\n"
     ]
    }
   ],
   "source": [
    "src_vocab_size = len(src_vocab)+1\n",
    "tar_vocab_size = len(tar_vocab)+1\n",
    "print(src_vocab_size)\n",
    "print(tar_vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "영어와 프랑스어는 각각 약 80개와 100개의 글자가 존재합니다. 이 중에서 인덱스를 임의로 부여하여 일부만 출력해봅시다. 현 상태에서 인덱스를 사용하려고 하면 에러가 납니다. 하지만 정렬하여 순서를 정해준 뒤에 인덱스를 사용하여 출력해주면 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
      "['T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w']\n"
     ]
    }
   ],
   "source": [
    "src_vocab = sorted(list(src_vocab))\n",
    "tar_vocab = sorted(list(tar_vocab))\n",
    "print(src_vocab[45:75])\n",
    "print(tar_vocab[45:75])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{' ': 1, '!': 2, '\"': 3, '$': 4, '%': 5, '&': 6, \"'\": 7, ',': 8, '-': 9, '.': 10, '/': 11, '0': 12, '1': 13, '2': 14, '3': 15, '4': 16, '5': 17, '6': 18, '7': 19, '8': 20, '9': 21, ':': 22, '?': 23, 'A': 24, 'B': 25, 'C': 26, 'D': 27, 'E': 28, 'F': 29, 'G': 30, 'H': 31, 'I': 32, 'J': 33, 'K': 34, 'L': 35, 'M': 36, 'N': 37, 'O': 38, 'P': 39, 'Q': 40, 'R': 41, 'S': 42, 'T': 43, 'U': 44, 'V': 45, 'W': 46, 'X': 47, 'Y': 48, 'Z': 49, 'a': 50, 'b': 51, 'c': 52, 'd': 53, 'e': 54, 'f': 55, 'g': 56, 'h': 57, 'i': 58, 'j': 59, 'k': 60, 'l': 61, 'm': 62, 'n': 63, 'o': 64, 'p': 65, 'q': 66, 'r': 67, 's': 68, 't': 69, 'u': 70, 'v': 71, 'w': 72, 'x': 73, 'y': 74, 'z': 75, 'é': 76, '’': 77, '€': 78}\n",
      "{'\\t': 1, '\\n': 2, ' ': 3, '!': 4, '\"': 5, '$': 6, '%': 7, '&': 8, \"'\": 9, '(': 10, ')': 11, ',': 12, '-': 13, '.': 14, '0': 15, '1': 16, '2': 17, '3': 18, '4': 19, '5': 20, '6': 21, '7': 22, '8': 23, '9': 24, ':': 25, '?': 26, 'A': 27, 'B': 28, 'C': 29, 'D': 30, 'E': 31, 'F': 32, 'G': 33, 'H': 34, 'I': 35, 'J': 36, 'K': 37, 'L': 38, 'M': 39, 'N': 40, 'O': 41, 'P': 42, 'Q': 43, 'R': 44, 'S': 45, 'T': 46, 'U': 47, 'V': 48, 'W': 49, 'X': 50, 'Y': 51, 'Z': 52, 'a': 53, 'b': 54, 'c': 55, 'd': 56, 'e': 57, 'f': 58, 'g': 59, 'h': 60, 'i': 61, 'j': 62, 'k': 63, 'l': 64, 'm': 65, 'n': 66, 'o': 67, 'p': 68, 'q': 69, 'r': 70, 's': 71, 't': 72, 'u': 73, 'v': 74, 'w': 75, 'x': 76, 'y': 77, 'z': 78, '\\xa0': 79, '«': 80, '»': 81, 'À': 82, 'Ç': 83, 'É': 84, 'Ê': 85, 'Ô': 86, 'à': 87, 'â': 88, 'ç': 89, 'è': 90, 'é': 91, 'ê': 92, 'ë': 93, 'î': 94, 'ï': 95, 'ô': 96, 'ù': 97, 'û': 98, 'œ': 99, 'С': 100, '\\u2009': 101, '\\u200b': 102, '‘': 103, '’': 104, '\\u202f': 105}\n"
     ]
    }
   ],
   "source": [
    "src_to_index = dict([(word, i+1) for i, word in enumerate(src_vocab)])\n",
    "tar_to_index = dict([(word, i+1) for i, word in enumerate(tar_vocab)])\n",
    "print(src_to_index)\n",
    "print(tar_to_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 인덱스가 부여된 글자 집합으로부터 갖고있는 훈련 데이터에 정수 인코딩을 수행하겠습니다. 우선 인코더의 입력이 될 영어 문장 샘플에 대해서 정수 인코딩을 수행해보고, 5개의 샘플을 출력해봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[30, 64, 10], [31, 58, 10], [31, 58, 10], [41, 70, 63, 2], [41, 70, 63, 2]]\n"
     ]
    }
   ],
   "source": [
    "encoder_input = []\n",
    "for line in lines.src: # 1줄씩 읽음\n",
    "    temp_x = []\n",
    "    for w in line: # 1 글자씩 읽음\n",
    "        temp_x.append(src_to_index[w]) # 글자에 해당되는 정수로 변환\n",
    "    encoder_input.append(temp_x)\n",
    "print(encoder_input[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 3, 48, 53, 3, 4, 3, 2], [1, 3, 45, 53, 64, 73, 72, 3, 4, 3, 2], [1, 3, 45, 53, 64, 73, 72, 14, 3, 2], [1, 3, 29, 67, 73, 70, 71, 105, 4, 3, 2], [1, 3, 29, 67, 73, 70, 57, 78, 105, 4, 3, 2]]\n"
     ]
    }
   ],
   "source": [
    "# 디코더의 입력이 될 프랑스어 데이터에 대해서 정수 인코딩을 수행해보겠습니다.\n",
    "\n",
    "decoder_input = []\n",
    "for line in lines.tar:\n",
    "    temp_X = []\n",
    "    for w in line:\n",
    "        temp_X.append(tar_to_index[w])\n",
    "    decoder_input.append(temp_X)\n",
    "print(decoder_input[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "인코더 인풋과 디코더 인풋이 정상적으로 정수 인코딩이 되었습니다. 다만 아직 하나가 더 남았습니다. 디코더의 예측값과 비교하기 위한 디코더 타겟값이 필요합니다. 다만 이 실제값에는 시작 심볼에 해당하는 \\<sos>가 있을 필요가 없습니다. 따라서 모든 프랑스어 문장의 맨 앞에 있는 '\\t' 를 제거하도록 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3, 48, 53, 3, 4, 3, 2], [3, 45, 53, 64, 73, 72, 3, 4, 3, 2], [3, 45, 53, 64, 73, 72, 14, 3, 2], [3, 29, 67, 73, 70, 71, 105, 4, 3, 2], [3, 29, 67, 73, 70, 57, 78, 105, 4, 3, 2]]\n"
     ]
    }
   ],
   "source": [
    "decoder_target = []\n",
    "for line in lines.tar:\n",
    "    t=0\n",
    "    temp_X = []\n",
    "    for w in line:\n",
    "        if t>0: # 첫 번째 글자는 제외한다.\n",
    "            temp_X.append(tar_to_index[w])\n",
    "        t=t+1\n",
    "    decoder_target.append(temp_X)\n",
    "print(decoder_target[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 모든 데이터에 대해서 정수 인덱스로 변경하였으니 패딩 작업을 수행하겠습니다. 패딩을 위해서 영어 문장과 프랑스어 문장 각각에 대해서 가장 길이가 긴 샘플의 길이를 알아보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24\n",
      "76\n"
     ]
    }
   ],
   "source": [
    "max_src_len = max([len(line) for line in lines.src])\n",
    "max_tar_len = max([len(line) for line in lines.tar])\n",
    "print(max_src_len)\n",
    "print(max_tar_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "각각 25와 76의 길이를 가집니다. 이번 병렬 데이터는 영어와 프랑스어의 길이는 하나의 쌍이라고 하더라도 전부 다르므로 패딩을 할 때도 이 두 개의 데이터의 길이를 전부 동일하게 맞춰줄 필요는 없습니다. 영어 데이터는 영어 샘플들끼리, 프랑스어는 프랑스어 샘플들끼리 길이를 맞추어서 패딩하면 됩니다. 여기서는 가장 긴 샘플의 길이에 맞춰서 영어 데이터의 샘플은 전부 길이가 25가 되도록 패딩하고, 프랑스어 데이터의 샘플은 전부 길이가 76이 되도록 패딩합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input = pad_sequences(encoder_input, maxlen=max_src_len, padding='post')\n",
    "decoder_input = pad_sequences(decoder_input, maxlen=max_tar_len, padding='post')\n",
    "decoder_target = pad_sequences(decoder_target, maxlen=max_tar_len, padding='post')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 모든 값에 대해서 원-핫 인코딩을 수행합니다. 글자 단위 번역기므로 워드 임베딩은 별도로 사용되지 않으며, 예측값과의 오차 측정에 사용되는 실제값뿐만 아니라 입력값도 원-핫 벡터를 사용하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input = to_categorical(encoder_input)\n",
    "decoder_input = to_categorical(decoder_input)\n",
    "decoder_target = to_categorical(decoder_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 데이터에 대한 전처리가 모두 끝났습니다. 본격적으로 seq2seq 모델을 설계해보겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2) 교사 강요(Teacher Forcing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "모델을 설계하기 전에 혹시 의아한 점은 없으신가요? 현재 시점의 디코더 셀의 입력은 오직 이전 디코더 셀의 출력을 입력으로 받는다고 설명하였는데 decoder_input이 왜 필요할까요?\n",
    "\n",
    "훈련 과정에서는 이전 시점의 디코더 셀의 출력을 현재 시점의 디코더 셀의 입력으로 넣어주지 않고, 이전 시점의 실제값을 현재 시점의 디코더 셀의 입력값으로 하는 방법을 사용할 겁니다. 그 이유는 이전 시점의 디코더 셀의 예측이 틀렸는데 이를 현재 시점의 디코더 셀의 입력으로 사용하면 현재 시점의 디코더 셀의 예측도 잘못될 가능성이 높고 이는 연쇄 작용으로 디코더 전체의 예측을 어렵게 합니다. 이런 상황이 반복되면 훈련 시간이 느려집니다. 만약 이 상황을 원하지 않는다면 이전 시점의 디코더 셀의 예측값 대신 실제값을 현재 시점의 디코더 셀의 입력으로 사용하는 방법을 사용할 수 있습니다. 이와 같이 RNN의 모든 시점에 대해서 이전 시점의 예측값 대신 실제값을 입력으로 주는 방법을 교사 강요라고 합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3) seq2seq 기계 번역기 훈련시키기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_inputs = Input(shape=(None, src_vocab_size))\n",
    "encoder_lstm = LSTM(units=256, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder_lstm(encoder_inputs)\n",
    "# encoder_outputs를 같이 리턴받지만 여기서는 필요없기에 버리게 됩니다.\n",
    "encoder_states = [state_h, state_c]\n",
    "# LSTM은 바닐라 RNN과 달리 상태가 두 개. 은닉 상태와 셀 상태."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "인코더를 주목해보면 functional API를 사용한다는 것 외에는 앞서 다른 실습에서 본 LSTM 설계와 크게 다르지는 않습니다. 우선 LSTM의 은닉 상태 크기는 256으로 선택하였습니다. 인코더의 내부 상태를 디코더로 넘겨주어야 하기 때문에 return_state=True로 설정합니다. 이제 인코더에 입력을 넣으면 내부 상태를 리턴합니다.\n",
    "\n",
    "LSTM에서 state_h, state_c를 리턴받는데, 이는 각각 LSTM 챕터에서 배운 은닉 상태와 셀 상태에 해당됩니다. 앞서 이론을 설명할 때는 셀 상태는 설명에서 생략하고 은닉 상태만 언급하였으나 사실 LSTM은 은닉 상태와 셀 상태라는 두 가지 상태를 가진다는 사실을 기억해야 합니다. 갑자기 어려워진 게 아닙니다. 단지 은닉 상태만 전달하는 게 아니라 은닉 상태와 셀 상태 두 가지를 전달한다고 생각하면 됩니다. 이 두 가지 상태를 encoder_states에 저장합니다. encoder_states를 디코더에 전달하므로서 이 두 가지 상태 모두를 디코더로 전달합니다. 이것이 앞서 배운 컨텍스트 벡터입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_inputs = Input(shape=(None, tar_vocab_size))\n",
    "decoder_lstm = LSTM(units=256, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_inputs, initial_state=encoder_states)\n",
    "# 디코더의 첫 상태를 인코더의 은닉 상태, 셀 상태로 합니다.\n",
    "decoder_softmax_layer = Dense(tar_vocab_size, activation='softmax')\n",
    "decoder_outputs = decoder_softmax_layer(decoder_outputs)\n",
    "\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "디코더는 인코더의 마지막 은닉 상태를 초기 은닉 상태로 사용합니다. 위에서 initial_state의 인자값으로 encoder_states를 주는 코드가 이에 해당됩니다. 또한 동일하게 디코더의 은닉 상태 크기도 256으로 주었습니다. 디코더도 은닉 상태, 셀 상태를 리턴하기는 하지만 훈련 과정에서는 사용하지 않습니다. 그 후 출력층에 프랑스어의 단어 집합의 크기만큼 뉴런을 배치한 후 소프트맥스 함수를 사용하여 실제값과의 오차를 구합니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "750/750 [==============================] - 132s 176ms/step - loss: 0.7721 - val_loss: 0.6927\n",
      "Epoch 2/50\n",
      "750/750 [==============================] - 131s 175ms/step - loss: 0.4911 - val_loss: 0.5636\n",
      "Epoch 3/50\n",
      "750/750 [==============================] - 132s 176ms/step - loss: 0.4130 - val_loss: 0.4998\n",
      "Epoch 4/50\n",
      "750/750 [==============================] - 133s 177ms/step - loss: 0.3665 - val_loss: 0.4583\n",
      "Epoch 5/50\n",
      "750/750 [==============================] - 133s 177ms/step - loss: 0.3352 - val_loss: 0.4334\n",
      "Epoch 6/50\n",
      "750/750 [==============================] - 133s 178ms/step - loss: 0.3116 - val_loss: 0.4110\n",
      "Epoch 7/50\n",
      "750/750 [==============================] - 132s 176ms/step - loss: 0.2933 - val_loss: 0.3948\n",
      "Epoch 8/50\n",
      "750/750 [==============================] - 132s 177ms/step - loss: 0.2786 - val_loss: 0.3832\n",
      "Epoch 9/50\n",
      "750/750 [==============================] - 133s 177ms/step - loss: 0.2664 - val_loss: 0.3756\n",
      "Epoch 10/50\n",
      "750/750 [==============================] - 133s 177ms/step - loss: 0.2560 - val_loss: 0.3697\n",
      "Epoch 11/50\n",
      "750/750 [==============================] - 133s 177ms/step - loss: 0.2469 - val_loss: 0.3644\n",
      "Epoch 12/50\n",
      "750/750 [==============================] - 133s 177ms/step - loss: 0.2391 - val_loss: 0.3597\n",
      "Epoch 13/50\n",
      "750/750 [==============================] - 133s 177ms/step - loss: 0.2319 - val_loss: 0.3585\n",
      "Epoch 14/50\n",
      "750/750 [==============================] - 134s 178ms/step - loss: 0.2255 - val_loss: 0.3561\n",
      "Epoch 15/50\n",
      "750/750 [==============================] - 133s 178ms/step - loss: 0.2195 - val_loss: 0.3541\n",
      "Epoch 16/50\n",
      "750/750 [==============================] - 133s 178ms/step - loss: 0.2140 - val_loss: 0.3535\n",
      "Epoch 17/50\n",
      "750/750 [==============================] - 133s 177ms/step - loss: 0.2089 - val_loss: 0.3533\n",
      "Epoch 18/50\n",
      "750/750 [==============================] - 133s 177ms/step - loss: 0.2042 - val_loss: 0.3526\n",
      "Epoch 19/50\n",
      "750/750 [==============================] - 133s 178ms/step - loss: 0.1998 - val_loss: 0.3543\n",
      "Epoch 20/50\n",
      "750/750 [==============================] - 134s 178ms/step - loss: 0.1957 - val_loss: 0.3542\n",
      "Epoch 21/50\n",
      "750/750 [==============================] - 133s 177ms/step - loss: 0.1918 - val_loss: 0.3560\n",
      "Epoch 22/50\n",
      "750/750 [==============================] - 133s 178ms/step - loss: 0.1881 - val_loss: 0.3570\n",
      "Epoch 23/50\n",
      "750/750 [==============================] - 134s 179ms/step - loss: 0.1846 - val_loss: 0.3609\n",
      "Epoch 24/50\n",
      "750/750 [==============================] - 133s 177ms/step - loss: 0.1812 - val_loss: 0.3597\n",
      "Epoch 25/50\n",
      "750/750 [==============================] - 134s 179ms/step - loss: 0.1781 - val_loss: 0.3619\n",
      "Epoch 26/50\n",
      "750/750 [==============================] - 133s 177ms/step - loss: 0.1750 - val_loss: 0.3652\n",
      "Epoch 27/50\n",
      "750/750 [==============================] - 133s 177ms/step - loss: 0.1722 - val_loss: 0.3664\n",
      "Epoch 28/50\n",
      "750/750 [==============================] - 132s 177ms/step - loss: 0.1694 - val_loss: 0.3674\n",
      "Epoch 29/50\n",
      "750/750 [==============================] - 132s 176ms/step - loss: 0.1667 - val_loss: 0.3727\n",
      "Epoch 30/50\n",
      "750/750 [==============================] - 133s 177ms/step - loss: 0.1643 - val_loss: 0.3721\n",
      "Epoch 31/50\n",
      "750/750 [==============================] - 133s 178ms/step - loss: 0.1619 - val_loss: 0.3761\n",
      "Epoch 32/50\n",
      "750/750 [==============================] - 133s 177ms/step - loss: 0.1595 - val_loss: 0.3766\n",
      "Epoch 33/50\n",
      "750/750 [==============================] - 132s 176ms/step - loss: 0.1573 - val_loss: 0.3797\n",
      "Epoch 34/50\n",
      "750/750 [==============================] - 132s 177ms/step - loss: 0.1551 - val_loss: 0.3800\n",
      "Epoch 35/50\n",
      "750/750 [==============================] - 133s 177ms/step - loss: 0.1531 - val_loss: 0.3840\n",
      "Epoch 36/50\n",
      "750/750 [==============================] - 133s 178ms/step - loss: 0.1510 - val_loss: 0.3861\n",
      "Epoch 37/50\n",
      "750/750 [==============================] - 133s 177ms/step - loss: 0.1491 - val_loss: 0.3871\n",
      "Epoch 38/50\n",
      "750/750 [==============================] - 133s 177ms/step - loss: 0.1472 - val_loss: 0.3916\n",
      "Epoch 39/50\n",
      "750/750 [==============================] - 133s 178ms/step - loss: 0.1454 - val_loss: 0.3942\n",
      "Epoch 40/50\n",
      "750/750 [==============================] - 135s 181ms/step - loss: 0.1437 - val_loss: 0.3976\n",
      "Epoch 41/50\n",
      "750/750 [==============================] - 134s 179ms/step - loss: 0.1421 - val_loss: 0.3997\n",
      "Epoch 42/50\n",
      "750/750 [==============================] - 136s 181ms/step - loss: 0.1404 - val_loss: 0.4012\n",
      "Epoch 43/50\n",
      "750/750 [==============================] - 133s 178ms/step - loss: 0.1389 - val_loss: 0.4044\n",
      "Epoch 44/50\n",
      "750/750 [==============================] - 133s 178ms/step - loss: 0.1375 - val_loss: 0.4067\n",
      "Epoch 45/50\n",
      "750/750 [==============================] - 134s 178ms/step - loss: 0.1359 - val_loss: 0.4090\n",
      "Epoch 46/50\n",
      "750/750 [==============================] - 134s 178ms/step - loss: 0.1346 - val_loss: 0.4125\n",
      "Epoch 47/50\n",
      "750/750 [==============================] - 133s 178ms/step - loss: 0.1332 - val_loss: 0.4151\n",
      "Epoch 48/50\n",
      "750/750 [==============================] - 133s 178ms/step - loss: 0.1320 - val_loss: 0.4191\n",
      "Epoch 49/50\n",
      "750/750 [==============================] - 133s 178ms/step - loss: 0.1306 - val_loss: 0.4178\n",
      "Epoch 50/50\n",
      "750/750 [==============================] - 133s 178ms/step - loss: 0.1293 - val_loss: 0.4223\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x201dc1e3048>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x=[encoder_input, decoder_input],\n",
    "          y=decoder_target,\n",
    "          batch_size=64,\n",
    "          epochs=50,\n",
    "          validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "입력으로는 인코더 입력과 디코더 입력이 들어가고, 디코더의 실제값인 decoder_target도 필요합니다. 배치 크기는 64로 하였으며 총 50 에포크를 학습합니다. 위에서 설정한 은닉 상태의 크기와 에포크 수는 실제로는 훈련 데이터에 과적합 상태를 불러옵니다. 중간부터 검증 데이터에 대한 오차인 val_loss의 값이 올라가는데, 사실 이번 실습에서는 주어진 데이터의 양과 태스크의 특성으로 인해 훈련 과정에서 훈련 데이터의 정확도와 과적합 방지라는 두 마리 토끼를 동시에 잡기에는 쉽지 않습니다. 여기서는 우선 seq2seq의 메커니즘과 짧은 문장과 긴 문장에 대한 성능 차이에 대한 확인을 중점으로 두고 훈련 데이터에 과적합 된 상태로 동작 단계로 넘어갑니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4) seq2seq 기계 번역기 동작시키기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "앞서 seq2seq는 훈련할 때와 동작할 때의 방식이 다르다고 언급한 바 있습니다. 이번에는 입력한 문장에 대해서 기계 번역을 하도록 모델을 조정하고 동작시켜보도록 하겠습니다.\n",
    "\n",
    "전체적인 번역 동작 단계를 정리하면 아래와 같습니다.\n",
    "1. 번역하고자 하는 입력 문장이 인코더에 들어가서 은닉 상태와 셀 상태를 얻습니다.\n",
    "2. 상태와 \\<SOS>에 해당하는 '\\t'를 디코더로 보냅니다.\n",
    "3. 디코더가 \\<EOS>에 해당하는 '\\n'이 나올 때까지 다음 문자를 예측하는 행동을 반복합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_model = Model(inputs=encoder_inputs,\n",
    "                      outputs=encoder_states)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "우선 인코더를 정의합니다. encoder_inputs와 encoder_states는 훈련 과정에서 이미 정의한 것들을 재사용하는 것입니다. 이제 디코더를 설계해보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이전 시점의 상태들을 저장하는 텐서\n",
    "decoder_state_input_h = Input(shape=(256,))\n",
    "decoder_state_input_c = Input(shape=(256,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "decoder_outputs, state_h, state_c = decoder_lstm(decoder_inputs,\n",
    "                                                 initial_state=decoder_states_inputs)\n",
    "# 문장의 다음 단어를 예측하기 위해서 초기 상태(initial_state)를 이전 시점의 상태로 사용.\n",
    "#이는 뒤의 함수 decode_sequence()에 구현\n",
    "\n",
    "decoder_states = [state_h, state_c]\n",
    "# 훈련 과정에서와 달리 LSTM의 리턴하는 은닉 상태와 셀 상태인 state_h와 state_c를 버리지 않음.\n",
    "decoder_outputs = decoder_softmax_layer(decoder_outputs)\n",
    "decoder_model = Model(inputs=[decoder_inputs] + decoder_states_inputs,\n",
    "                      outputs=[decoder_outputs] + decoder_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_to_src = dict((i, char) for char, i in src_to_index.items())\n",
    "index_to_tar = dict((i, char) for char, i in tar_to_index.items())\n",
    "\n",
    "#단어로부터 인덱스를 얻는 것이 아니라 인덱스로부터 단어를 얻을 수 있는 index_to_src와 index_to_tar를 만들었습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequence(input_seq):\n",
    "    # 입력으로부터 인코더의 상태를 얻음\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "\n",
    "    # <SOS>에 해당하는 원-핫 벡터 생성\n",
    "    target_seq = np.zeros((1, 1, tar_vocab_size))\n",
    "    target_seq[0, 0, tar_to_index['\\t']] = 1.\n",
    "\n",
    "    stop_condition = False\n",
    "    decoded_sentence = \"\"\n",
    "\n",
    "    # stop_condition이 True가 될 때까지 루프 반복\n",
    "    while not stop_condition:\n",
    "        # 이점 시점의 상태 states_value를 현 시점의 초기 상태로 사용\n",
    "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
    "\n",
    "        # 예측 결과를 문자로 변환\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_char = index_to_tar[sampled_token_index]\n",
    "\n",
    "        # 현재 시점의 예측 문자를 예측 문장에 추가\n",
    "        decoded_sentence += sampled_char\n",
    "\n",
    "        # <eos>에 도달하거나 최대 길이를 넘으면 중단.\n",
    "        if (sampled_char == '\\n' or\n",
    "           len(decoded_sentence) > max_tar_len):\n",
    "            stop_condition = True\n",
    "\n",
    "        # 현재 시점의 예측 결과를 다음 시점의 입력으로 사용하기 위해 저장\n",
    "        target_seq = np.zeros((1, 1, tar_vocab_size))\n",
    "        target_seq[0, 0, sampled_token_index] = 1.\n",
    "\n",
    "        # 현재 시점의 상태를 다음 시점의 상태로 사용하기 위해 저장\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------\n",
      "입력 문장: Run!\n",
      "정답 문장:  Cours ! \n",
      "번역기가 번역한 문장:  Cours-le ! \n",
      "-----------------------------------\n",
      "입력 문장: I left.\n",
      "정답 문장:  Je suis partie. \n",
      "번역기가 번역한 문장:  Je suis reposé. \n",
      "-----------------------------------\n",
      "입력 문장: Call us.\n",
      "정답 문장:  Appelez-nous ! \n",
      "번역기가 번역한 문장:  Appelez-moi ! \n",
      "-----------------------------------\n",
      "입력 문장: How nice!\n",
      "정답 문장:  Comme c'est gentil ! \n",
      "번역기가 번역한 문장:  Comme c'est chouette ! \n",
      "-----------------------------------\n",
      "입력 문장: Turn left.\n",
      "정답 문장:  Tourne à gauche. \n",
      "번역기가 번역한 문장:  Tournez à donner. \n"
     ]
    }
   ],
   "source": [
    "for seq_index in [3,50,100,300,1001]: # 입력 문장의 인덱스\n",
    "    input_seq = encoder_input[seq_index: seq_index + 1]\n",
    "    decoded_sentence = decode_sequence(input_seq)\n",
    "    print(35 * \"-\")\n",
    "    print('입력 문장:', lines.src[seq_index])\n",
    "    print('정답 문장:', lines.tar[seq_index][1:len(lines.tar[seq_index])-1]) # '\\t'와 '\\n'을 빼고 출력\n",
    "    print('번역기가 번역한 문장:', decoded_sentence[:len(decoded_sentence)-1]) # '\\n'을 빼고 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10.3.  영어 한국어 Word2Vec 실습"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "gensim 패키지에 Word2Vec은 이미 구현되어 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.3.1. English"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.3.1.1. Data Understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "링크 : https://wit3.fbk.eu/get.php?path=XML_releases/xml/ted_en-20160408.zip&filename=ted_en-20160408.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "xml 파일을 가져옵니다. 전처리를 해야 합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.3.1.2. Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from lxml import etree\n",
    "import urllib.request\n",
    "import zipfile\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "urllib.request.urlretrieve(\"https://wit3.fbk.eu/get.php?path=XML_releases/xml/ted_en-20160408.zip&filename=ted_en-20160408.zip\", filename=\"ted_en-20160408.zip\")\n",
    "# 데이터 다운로드\n",
    "\n",
    "with zipfile.ZipFile('ted_en-20160408.zip', 'r') as z:\n",
    "    target_text = etree.parse(z.open('ted_en-20160408.xml', 'r'))\n",
    "    parse_text = '\\n'.join(target_text.xpath('//content/text()'))\n",
    "# xml 파일로부터 <content>와 </content> 사이의 내용만 가져온다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Here are two reasons companies fail: they only do more of the same, or they only do what's new.\\nTo me the real, real solution to quality growth is figuring out the balance between two activities: exploration and exploitation. Both are necessary, but it can be too much of a good thing.\\nConsider Facit\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parse_text[:300]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "전처리를 수행해봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 정규표현식을 통해 기타 문자들 제거\n",
    "content_text = re.sub(r'\\([^)]*\\)', '', parse_text)\n",
    "\n",
    "# nltk를 이용하여 문장 토큰화\n",
    "sent_text = sent_tokenize(content_text)\n",
    "\n",
    "# 각 문장의 구두점 제거, 소문자로 변환\n",
    "normalized_text = []\n",
    "for string in sent_text:\n",
    "    tokens = re.sub(r\"[^a-z0-9]+\", \" \", string.lower())\n",
    "    normalized_text.append(tokens)\n",
    "    \n",
    "# 각 문장에 대해서 nltk 이용하여 단어 토큰화 수행\n",
    "result = [word_tokenize(sentence) for sentence in normalized_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "273424\n"
     ]
    }
   ],
   "source": [
    "# 총 샘플의 개수\n",
    "print(len(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['here', 'are', 'two', 'reasons', 'companies', 'fail', 'they', 'only', 'do', 'more', 'of', 'the', 'same', 'or', 'they', 'only', 'do', 'what', 's', 'new']\n",
      "['to', 'me', 'the', 'real', 'real', 'solution', 'to', 'quality', 'growth', 'is', 'figuring', 'out', 'the', 'balance', 'between', 'two', 'activities', 'exploration', 'and', 'exploitation']\n",
      "['both', 'are', 'necessary', 'but', 'it', 'can', 'be', 'too', 'much', 'of', 'a', 'good', 'thing']\n"
     ]
    }
   ],
   "source": [
    "# 샘플 3개만 추출\n",
    "\n",
    "for line in result[:3]:\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.3.1.3. Training Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec(sentences=result,\n",
    "                size=100, # 워드 벡터의 특징 값. 즉, 임베딩 된 벡터의 차원.\n",
    "                window=5, # 컨텍스트 윈도우 크기.\n",
    "                min_count=5, # 단어 최소 빈도 수 제한(빈도가 적은 단어들은 학습하지 않는다.)\n",
    "                workers=4, # 학습을 위한 프로세스 수\n",
    "                sg=0) # 0은 CBoW, 1은 Skip-gram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word2Vec은 입력한 단어에 대해 가장 유사한 단어들을 출력하는 model.wv.most_similar를 지원합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('woman', 0.8585288524627686), ('guy', 0.8139721155166626), ('lady', 0.7843915820121765), ('girl', 0.7614036798477173), ('boy', 0.7594757080078125), ('gentleman', 0.7481688261032104), ('soldier', 0.7418292760848999), ('kid', 0.710865318775177), ('poet', 0.7055411338806152), ('writer', 0.6627633571624756)]\n"
     ]
    }
   ],
   "source": [
    "model_result = model.wv.most_similar(\"man\")\n",
    "print(model_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.3.1.4. Model save and load"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "공들여 학습한 모델은 언제든 나중에 다시 사용할 수 있도록 컴퓨터 파일로 저장하고 다시 로드해보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.save_word2vec_format('eng_w2v') # 모델 저장\n",
    "loaded_model = KeyedVectors.load_word2vec_format(\"eng_w2v\") #모델 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('man', 0.858528733253479), ('girl', 0.8523677587509155), ('lady', 0.8222741484642029), ('boy', 0.7946901321411133), ('kid', 0.7542577981948853), ('child', 0.7482434511184692), ('guy', 0.7270610332489014), ('soldier', 0.7268799543380737), ('gentleman', 0.7152020931243896), ('poet', 0.6952031850814819)]\n"
     ]
    }
   ],
   "source": [
    "# 로드한 모델로 출력 해보겠습니다.\n",
    "model_result = loaded_model.most_similar(\"woman\")\n",
    "print(model_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.3.2. Korean Word2Vec_네이버영화리뷰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "from konlpy.tag import Okt\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "네이버 영화 리뷰를 해보겠습니다!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('ratings.txt', <http.client.HTTPMessage at 0x1f7df5d8240>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/e9t/nsmc/master/ratings.txt\", filename=\"ratings.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_table(\"ratings.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>document</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8112052</td>\n",
       "      <td>어릴때보고 지금다시봐도 재밌어요ㅋㅋ</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8132799</td>\n",
       "      <td>디자인을 배우는 학생으로, 외국디자이너와 그들이 일군 전통을 통해 발전해가는 문화산...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4655635</td>\n",
       "      <td>폴리스스토리 시리즈는 1부터 뉴까지 버릴께 하나도 없음.. 최고.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9251303</td>\n",
       "      <td>와.. 연기가 진짜 개쩔구나.. 지루할거라고 생각했는데 몰입해서 봤다.. 그래 이런...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10067386</td>\n",
       "      <td>안개 자욱한 밤하늘에 떠 있는 초승달 같은 영화.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id                                           document  label\n",
       "0   8112052                                어릴때보고 지금다시봐도 재밌어요ㅋㅋ      1\n",
       "1   8132799  디자인을 배우는 학생으로, 외국디자이너와 그들이 일군 전통을 통해 발전해가는 문화산...      1\n",
       "2   4655635               폴리스스토리 시리즈는 1부터 뉴까지 버릴께 하나도 없음.. 최고.      1\n",
       "3   9251303  와.. 연기가 진짜 개쩔구나.. 지루할거라고 생각했는데 몰입해서 봤다.. 그래 이런...      1\n",
       "4  10067386                        안개 자욱한 밤하늘에 떠 있는 초승달 같은 영화.      1"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200000\n"
     ]
    }
   ],
   "source": [
    "print(len(train_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "# NULL값 존재 유무\n",
    "print(train_data.isnull().values.any())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "train_data = train_data.dropna(how='any')\n",
    "print(train_data.isnull().values.any())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "199992\n"
     ]
    }
   ],
   "source": [
    "print(len(train_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['document'] = train_data['document'].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\",\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>document</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8112052</td>\n",
       "      <td>어릴때보고 지금다시봐도 재밌어요ㅋㅋ</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8132799</td>\n",
       "      <td>디자인을 배우는 학생으로 외국디자이너와 그들이 일군 전통을 통해 발전해가는 문화산업...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4655635</td>\n",
       "      <td>폴리스스토리 시리즈는 부터 뉴까지 버릴께 하나도 없음 최고</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9251303</td>\n",
       "      <td>와 연기가 진짜 개쩔구나 지루할거라고 생각했는데 몰입해서 봤다 그래 이런게 진짜 영화지</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10067386</td>\n",
       "      <td>안개 자욱한 밤하늘에 떠 있는 초승달 같은 영화</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id                                           document  label\n",
       "0   8112052                                어릴때보고 지금다시봐도 재밌어요ㅋㅋ      1\n",
       "1   8132799  디자인을 배우는 학생으로 외국디자이너와 그들이 일군 전통을 통해 발전해가는 문화산업...      1\n",
       "2   4655635                   폴리스스토리 시리즈는 부터 뉴까지 버릴께 하나도 없음 최고      1\n",
       "3   9251303   와 연기가 진짜 개쩔구나 지루할거라고 생각했는데 몰입해서 봤다 그래 이런게 진짜 영화지      1\n",
       "4  10067386                         안개 자욱한 밤하늘에 떠 있는 초승달 같은 영화      1"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 불용어 정의\n",
    "stopwords = ['의','가','이','은','들','는','좀','잘','걍','과','도','를','으로','자','에','와','한','하다']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  토큰화\n",
    "\n",
    "okt = Okt()\n",
    "tokenized_data = []\n",
    "for sentence in train_data['document']:\n",
    "    temp_x = okt.morphs(sentence, stem=True) # 토큰화\n",
    "    temp_x = [word for word in temp_x if not word in stopwords] # 불용어 제거\n",
    "    tokenized_data.append(temp_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "리뷰의 최대 길이 : 72\n",
      "리뷰의 평균 길이 : 10.716703668146726\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 리뷰 길이 분포 확인\n",
    "print('리뷰의 최대 길이 :',max(len(l) for l in tokenized_data))\n",
    "print('리뷰의 평균 길이 :',sum(map(len, tokenized_data))/len(tokenized_data))\n",
    "plt.hist([len(s) for s in tokenized_data], bins=50)\n",
    "plt.xlabel('length of samples')\n",
    "plt.ylabel('number of samples')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec(sentences = tokenized_data,\n",
    "                size = 100,\n",
    "                window = 5,\n",
    "                min_count = 5,\n",
    "                workers = 4,\n",
    "                sg = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16477, 100)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 완성된 임베딩 매트릭스의 크기 확인.\n",
    "\n",
    "model.wv.vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('한석규', 0.8573399782180786), ('최민수', 0.8522400856018066), ('안성기', 0.8446011543273926), ('박중훈', 0.8420392274856567), ('이민호', 0.8398627042770386), ('설경구', 0.8321352005004883), ('김수현', 0.8254207968711853), ('송강호', 0.8220459222793579), ('윤제문', 0.8211894631385803), ('이미숙', 0.8202099204063416)]\n"
     ]
    }
   ],
   "source": [
    "print(model.wv.most_similar(\"최민식\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('무협', 0.8910517692565918), ('느와르', 0.8649859428405762), ('슬래셔', 0.8614627122879028), ('블록버스터', 0.8577938079833984), ('정통', 0.8504294157028198), ('히어로', 0.8452297449111938), ('멜로', 0.8406030535697937), ('컬트', 0.8243943452835083), ('판타지', 0.8241962194442749), ('무비', 0.8210760354995728)]\n"
     ]
    }
   ],
   "source": [
    "print(model.wv.most_similar(\"호러\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.3.3. Korean Word2Vec_위키피디아"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.3.3.1. 위키피디아 한국어 덤프 파일 다운로드"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://dumps.wikimedia.org/kowiki/latest/\n",
    "\n",
    "여기서 kowiki-latest-pages-articles.xml.bz2 파일입니다. 해당 파일은 xml 파일므로, Word2Vec을 원활하게 진행하기 위해 파일 형식을 변환해줄 필요가 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.3.3.2. 위키피디아 익스트랙터 다운로드"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cmd 에서\n",
    "\n",
    "git clone \"https://github.com/attardi/wikiextractor.git\"\n",
    "\n",
    "으로 클로닝해줍니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.3.3.3. 위키피디아 한국어 덤프 파일 변환"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "덤프 파일과 익스트랙터를 동일한 경로에 두고\n",
    "\n",
    "python WikiExtractor.py kowiki-latest-pages-articles.xml.bz2  \n",
    "\n",
    "cmd에서 해당 경로에서 위 명령어를 쓰면 약 10분 정도 덤프를 텍스트로 변환합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.3.3.4. 훈련 데이터 만들기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "우선 AA 디렉토리 안의 모든 파일인 wiki00 ~ wiki90에 대해서 wikiAA.txt로 통합해보겠습니다. 프롬프트에서 아래의 커맨드를 수행합니다.\n",
    "\n",
    "copy AA디렉토리의 경로\\wiki* wikiAA.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다른 디렉토리에 대해서도 동일하게 진행\n",
    "\n",
    "- copy AB디렉토리의 경로\\wiki* wikiAB.txt\n",
    "- copy AC디렉토리의 경로\\wiki* wikiAC.txt\n",
    "- copy AD디렉토리의 경로\\wiki* wikiAD.txt\n",
    "- copy AE디렉토리의 경로\\wiki* wikiAE.txt\n",
    "- copy AF디렉토리의 경로\\wiki* wikiAF.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그 여섯개도 하나로 합치기\n",
    "\n",
    "- copy 현재 디렉토리의 경로\\wikiA* wiki_data.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.3.3.5. 훈련 데이터 전처리 학기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(r'C:\\Users\\thinp\\python\\Study\\Beginning NLP with DeepLearning_20.07\\wikiextractor\\wiki_data.text',\n",
    "         encoding=\"utf8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1번째 줄: <doc id=\"5\" url=\"https://ko.wikipedia.org/wiki?curid=5\" title=\"지미 카터\">\n",
      "\n",
      "2번째 줄: 지미 카터\n",
      "\n",
      "3번째 줄: 제임스 얼 \"지미\" 카터 주니어(, 1924년 10월 1일 ~ )는 민주당 출신 미국 39번째 대통령 (1977년 ~ 1981년)이다.\n",
      "\n",
      "4번째 줄: 지미 카터는 조지아주 섬터 카운티 플레인스 마을에서 태어났다. 조지아 공과대학교를 졸업하였다. 그 후 해군에 들어가 전함·원자력·잠수함의 승무원으로 일하였다. 1953년 미국 해군 대위로 예편하였고 이후 땅콩·면화 등을 가꿔 많은 돈을 벌었다. 그의 별명이 \"땅콩 농부\" (Peanut Farmer)로 알려졌다.\n",
      "\n",
      "5번째 줄: 1962년 조지아 주 상원 의원 선거에서 낙선하나 그 선거가 부정선거 였음을 입증하게 되어 당선되고, 1966년 조지아 주 지사 선거에 낙선하지만 1970년 조지아 주 지사를 역임했다. 대통령이 되기 전 조지아주 상원의원을 두번 연임했으며, 1971년부터 1975년까지 조지아 지사로 근무했다. 조지아 주지사로 지내면서, 미국에 사는 흑인 등용법을 내세웠다.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "i=0\n",
    "while True:\n",
    "    line = f.readline()\n",
    "    if line != '\\n':\n",
    "        i +=1\n",
    "        print(\"%d번째 줄: \"%i + line)\n",
    "    if i ==5:\n",
    "        break\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "본격적인 전처리를 시작하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Okt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "okt = Okt()\n",
    "fread=open(r'C:\\Users\\thinp\\python\\Study\\Beginning NLP with DeepLearning_20.07\\wikiextractor\\wiki_data.text',\n",
    "         encoding=\"utf8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000 th while. \n",
      "10000 th while. \n",
      "15000 th while. \n",
      "20000 th while. \n",
      "25000 th while. \n",
      "30000 th while. \n",
      "35000 th while. \n",
      "40000 th while. \n",
      "45000 th while. \n",
      "50000 th while. \n",
      "55000 th while. \n",
      "60000 th while. \n",
      "65000 th while. \n",
      "70000 th while. \n",
      "75000 th while. \n",
      "80000 th while. \n",
      "85000 th while. \n",
      "90000 th while. \n",
      "95000 th while. \n",
      "100000 th while. \n",
      "105000 th while. \n",
      "110000 th while. \n",
      "115000 th while. \n",
      "120000 th while. \n",
      "125000 th while. \n",
      "130000 th while. \n",
      "135000 th while. \n",
      "140000 th while. \n",
      "145000 th while. \n",
      "150000 th while. \n",
      "155000 th while. \n",
      "160000 th while. \n",
      "165000 th while. \n",
      "170000 th while. \n",
      "175000 th while. \n",
      "180000 th while. \n",
      "185000 th while. \n",
      "190000 th while. \n",
      "195000 th while. \n",
      "200000 th while. \n",
      "205000 th while. \n",
      "210000 th while. \n",
      "215000 th while. \n",
      "220000 th while. \n",
      "225000 th while. \n",
      "230000 th while. \n",
      "235000 th while. \n",
      "240000 th while. \n",
      "245000 th while. \n",
      "250000 th while. \n",
      "255000 th while. \n",
      "260000 th while. \n",
      "265000 th while. \n",
      "270000 th while. \n",
      "275000 th while. \n",
      "280000 th while. \n",
      "285000 th while. \n",
      "290000 th while. \n",
      "295000 th while. \n",
      "300000 th while. \n",
      "305000 th while. \n",
      "310000 th while. \n",
      "315000 th while. \n",
      "320000 th while. \n",
      "325000 th while. \n",
      "330000 th while. \n",
      "335000 th while. \n",
      "340000 th while. \n",
      "345000 th while. \n",
      "350000 th while. \n",
      "355000 th while. \n",
      "360000 th while. \n",
      "365000 th while. \n",
      "370000 th while. \n",
      "375000 th while. \n",
      "380000 th while. \n",
      "385000 th while. \n",
      "390000 th while. \n",
      "395000 th while. \n",
      "400000 th while. \n",
      "405000 th while. \n",
      "410000 th while. \n",
      "415000 th while. \n",
      "420000 th while. \n",
      "425000 th while. \n",
      "430000 th while. \n",
      "435000 th while. \n",
      "440000 th while. \n",
      "445000 th while. \n",
      "450000 th while. \n",
      "455000 th while. \n",
      "460000 th while. \n",
      "465000 th while. \n",
      "470000 th while. \n",
      "475000 th while. \n",
      "480000 th while. \n",
      "485000 th while. \n",
      "490000 th while. \n",
      "495000 th while. \n",
      "500000 th while. \n",
      "505000 th while. \n",
      "510000 th while. \n",
      "515000 th while. \n",
      "520000 th while. \n",
      "525000 th while. \n",
      "530000 th while. \n",
      "535000 th while. \n",
      "540000 th while. \n",
      "545000 th while. \n",
      "550000 th while. \n",
      "555000 th while. \n",
      "560000 th while. \n",
      "565000 th while. \n",
      "570000 th while. \n",
      "575000 th while. \n",
      "580000 th while. \n",
      "585000 th while. \n",
      "590000 th while. \n",
      "595000 th while. \n",
      "600000 th while. \n",
      "605000 th while. \n",
      "610000 th while. \n",
      "615000 th while. \n",
      "620000 th while. \n",
      "625000 th while. \n",
      "630000 th while. \n",
      "635000 th while. \n",
      "640000 th while. \n",
      "645000 th while. \n",
      "650000 th while. \n",
      "655000 th while. \n",
      "660000 th while. \n",
      "665000 th while. \n",
      "670000 th while. \n",
      "675000 th while. \n",
      "680000 th while. \n",
      "685000 th while. \n",
      "690000 th while. \n",
      "695000 th while. \n",
      "700000 th while. \n",
      "705000 th while. \n",
      "710000 th while. \n",
      "715000 th while. \n",
      "720000 th while. \n",
      "725000 th while. \n",
      "730000 th while. \n",
      "735000 th while. \n",
      "740000 th while. \n",
      "745000 th while. \n",
      "750000 th while. \n",
      "755000 th while. \n",
      "760000 th while. \n",
      "765000 th while. \n",
      "770000 th while. \n",
      "775000 th while. \n",
      "780000 th while. \n",
      "785000 th while. \n",
      "790000 th while. \n",
      "795000 th while. \n",
      "800000 th while. \n",
      "805000 th while. \n",
      "810000 th while. \n",
      "815000 th while. \n",
      "820000 th while. \n",
      "825000 th while. \n",
      "830000 th while. \n",
      "835000 th while. \n",
      "840000 th while. \n",
      "845000 th while. \n",
      "850000 th while. \n",
      "855000 th while. \n",
      "860000 th while. \n",
      "865000 th while. \n",
      "870000 th while. \n",
      "875000 th while. \n",
      "880000 th while. \n",
      "885000 th while. \n",
      "890000 th while. \n",
      "895000 th while. \n",
      "900000 th while. \n",
      "905000 th while. \n",
      "910000 th while. \n",
      "915000 th while. \n",
      "920000 th while. \n",
      "925000 th while. \n",
      "930000 th while. \n",
      "935000 th while. \n",
      "940000 th while. \n",
      "945000 th while. \n",
      "950000 th while. \n",
      "955000 th while. \n",
      "960000 th while. \n",
      "965000 th while. \n",
      "970000 th while. \n",
      "975000 th while. \n",
      "980000 th while. \n",
      "985000 th while. \n",
      "990000 th while. \n",
      "995000 th while. \n",
      "1000000 th while. \n",
      "1005000 th while. \n",
      "1010000 th while. \n",
      "1015000 th while. \n",
      "1020000 th while. \n",
      "1025000 th while. \n",
      "1030000 th while. \n",
      "1035000 th while. \n",
      "1040000 th while. \n",
      "1045000 th while. \n",
      "1050000 th while. \n",
      "1055000 th while. \n",
      "1060000 th while. \n",
      "1065000 th while. \n",
      "1070000 th while. \n",
      "1075000 th while. \n",
      "1080000 th while. \n",
      "1085000 th while. \n",
      "1090000 th while. \n",
      "1095000 th while. \n",
      "1100000 th while. \n",
      "1105000 th while. \n",
      "1110000 th while. \n",
      "1115000 th while. \n",
      "1120000 th while. \n",
      "1125000 th while. \n",
      "1130000 th while. \n",
      "1135000 th while. \n",
      "1140000 th while. \n",
      "1145000 th while. \n",
      "1150000 th while. \n",
      "1155000 th while. \n",
      "1160000 th while. \n",
      "1165000 th while. \n",
      "1170000 th while. \n",
      "1175000 th while. \n",
      "1180000 th while. \n",
      "1185000 th while. \n",
      "1190000 th while. \n",
      "1195000 th while. \n",
      "1200000 th while. \n",
      "1205000 th while. \n",
      "1210000 th while. \n",
      "1215000 th while. \n",
      "1220000 th while. \n",
      "1225000 th while. \n",
      "1230000 th while. \n",
      "1235000 th while. \n",
      "1240000 th while. \n",
      "1245000 th while. \n",
      "1250000 th while. \n",
      "1255000 th while. \n",
      "1260000 th while. \n",
      "1265000 th while. \n",
      "1270000 th while. \n",
      "1275000 th while. \n",
      "1280000 th while. \n",
      "1285000 th while. \n",
      "1290000 th while. \n",
      "1295000 th while. \n",
      "1300000 th while. \n",
      "1305000 th while. \n",
      "1310000 th while. \n",
      "1315000 th while. \n",
      "1320000 th while. \n",
      "1325000 th while. \n",
      "1330000 th while. \n",
      "1335000 th while. \n",
      "1340000 th while. \n",
      "1345000 th while. \n",
      "1350000 th while. \n",
      "1355000 th while. \n",
      "1360000 th while. \n",
      "1365000 th while. \n",
      "1370000 th while. \n",
      "1375000 th while. \n",
      "1380000 th while. \n",
      "1385000 th while. \n",
      "1390000 th while. \n",
      "1395000 th while. \n",
      "1400000 th while. \n",
      "1405000 th while. \n",
      "1410000 th while. \n",
      "1415000 th while. \n",
      "1420000 th while. \n",
      "1425000 th while. \n",
      "1430000 th while. \n",
      "1435000 th while. \n",
      "1440000 th while. \n",
      "1445000 th while. \n",
      "1450000 th while. \n",
      "1455000 th while. \n",
      "1460000 th while. \n",
      "1465000 th while. \n",
      "1470000 th while. \n",
      "1475000 th while. \n",
      "1480000 th while. \n",
      "1485000 th while. \n",
      "1490000 th while. \n",
      "1495000 th while. \n",
      "1500000 th while. \n",
      "1505000 th while. \n",
      "1510000 th while. \n",
      "1515000 th while. \n",
      "1520000 th while. \n",
      "1525000 th while. \n",
      "1530000 th while. \n",
      "1535000 th while. \n",
      "1540000 th while. \n",
      "1545000 th while. \n",
      "1550000 th while. \n",
      "1555000 th while. \n",
      "1560000 th while. \n",
      "1565000 th while. \n",
      "1570000 th while. \n",
      "1575000 th while. \n",
      "1580000 th while. \n",
      "1585000 th while. \n",
      "1590000 th while. \n",
      "1595000 th while. \n",
      "1600000 th while. \n",
      "1605000 th while. \n",
      "1610000 th while. \n",
      "1615000 th while. \n",
      "1620000 th while. \n",
      "1625000 th while. \n",
      "1630000 th while. \n",
      "1635000 th while. \n",
      "1640000 th while. \n",
      "1645000 th while. \n",
      "1650000 th while. \n",
      "1655000 th while. \n",
      "1660000 th while. \n",
      "1665000 th while. \n",
      "1670000 th while. \n",
      "1675000 th while. \n",
      "1680000 th while. \n",
      "1685000 th while. \n",
      "1690000 th while. \n",
      "1695000 th while. \n",
      "1700000 th while. \n",
      "1705000 th while. \n",
      "1710000 th while. \n",
      "1715000 th while. \n",
      "1720000 th while. \n",
      "1725000 th while. \n",
      "1730000 th while. \n",
      "1735000 th while. \n",
      "1740000 th while. \n",
      "1745000 th while. \n",
      "1750000 th while. \n",
      "1755000 th while. \n",
      "1760000 th while. \n",
      "1765000 th while. \n",
      "1770000 th while. \n",
      "1775000 th while. \n",
      "1780000 th while. \n",
      "1785000 th while. \n",
      "1790000 th while. \n",
      "1795000 th while. \n",
      "1800000 th while. \n",
      "1805000 th while. \n",
      "1810000 th while. \n",
      "1815000 th while. \n",
      "1820000 th while. \n",
      "1825000 th while. \n",
      "1830000 th while. \n",
      "1835000 th while. \n",
      "1840000 th while. \n",
      "1845000 th while. \n",
      "1850000 th while. \n",
      "1855000 th while. \n",
      "1860000 th while. \n",
      "1865000 th while. \n",
      "1870000 th while. \n",
      "1875000 th while. \n",
      "1880000 th while. \n",
      "1885000 th while. \n",
      "1890000 th while. \n",
      "1895000 th while. \n",
      "1900000 th while. \n",
      "1905000 th while. \n",
      "1910000 th while. \n",
      "1915000 th while. \n",
      "1920000 th while. \n",
      "1925000 th while. \n",
      "1930000 th while. \n",
      "1935000 th while. \n",
      "1940000 th while. \n",
      "1945000 th while. \n",
      "1950000 th while. \n",
      "1955000 th while. \n",
      "1960000 th while. \n",
      "1965000 th while. \n",
      "1970000 th while. \n",
      "1975000 th while. \n",
      "1980000 th while. \n",
      "1985000 th while. \n",
      "1990000 th while. \n",
      "1995000 th while. \n",
      "2000000 th while. \n",
      "2005000 th while. \n",
      "2010000 th while. \n",
      "2015000 th while. \n",
      "2020000 th while. \n"
     ]
    }
   ],
   "source": [
    "n = 0\n",
    "result = []\n",
    "\n",
    "while True:\n",
    "    line = fread.readline() #  한 줄씩 리드\n",
    "    if not line:\n",
    "        break # 모두 읽으면 종료\n",
    "    n+=1\n",
    "    if n%5000 == 0: # 5,000의 배수로 while문이 실행될 때마다 몇 번째 while 문인지 출력\n",
    "        print(\"%d th while. \"%n)\n",
    "    \n",
    "    tokenlist = okt.pos(line, stem=True, norm=True) # 토큰화\n",
    "    temp = []\n",
    "    for word in tokenlist:\n",
    "        if word[1] in [\"Noun\"]: # 명사일 때만\n",
    "            temp.append((word[0])) # 해당 단어를 저장\n",
    "        \n",
    "    if temp: #명사가 존재하면\n",
    "        result.append(temp) # 결과에 저장\n",
    "        \n",
    "fread.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Count of Samples: {}\".format(len(result)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.3.3.6. Word2Vec Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec(result,\n",
    "                size=100,\n",
    "                window=5,\n",
    "                min_count=5,\n",
    "                workers=4,\n",
    "                sg=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.3.4. 사전 훈련된 Word2Vec 임베딩 소개"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "사전 훈련된 모델이 있다고 합니다. 가져와서 써도 좋습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "용량이 3기가가 넘기 때문에 돌려보진 않겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://drive.google.com/file/d/0B0ZXk88koS2KbDhXdWg1Q2RydlU/view\n",
    "\n",
    "박병규님의 한국어 프리트레인 모델입니다. 이것만 해보죠"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gensim.models.Word2Vec.load(r\"C:\\Users\\thinp\\python\\Study\\Beginning NLP with DeepLearning_20.07\\ko\\ko.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('고양이', 0.7290453314781189), ('거위', 0.7185634970664978), ('토끼', 0.7056223750114441), ('멧돼지', 0.6950401067733765), ('엄마', 0.693433403968811), ('난쟁이', 0.6806551218032837), ('한마리', 0.6770296096801758), ('아가씨', 0.675035297870636), ('아빠', 0.6729634404182434), ('목걸이', 0.6512461304664612)]\n"
     ]
    }
   ],
   "source": [
    "result = model.wv.most_similar(\"강아지\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10.4. Word2Vec 구현하기(Skip-Gram)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이번 챕터에서는 네거티브 샘플링을 사용하는 Word2Vec을 케라스로 구현해봅니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.4.1. 네거티브 샘플링을 사용한 Skip-Gram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://jalammar.github.io/illustrated-word2vec/\n",
    "\n",
    "이론은 여기서 보시면 됩니다. 영어주의입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.4.2. 20뉴스그룹 데이터 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11314\n"
     ]
    }
   ],
   "source": [
    "# 하나의 샘플에 최소 단어 2개는 있어야 합니다.\n",
    "# 그래야 중심단어, 주변단어의 관계가 성립합니다.\n",
    "# 이를 만족하지 않는 샘플은 전처리에서 제거합니다.\n",
    "\n",
    "dataset = fetch_20newsgroups(shuffle = True,\n",
    "                            random_state=1,\n",
    "                            remove=('headers',\n",
    "                                   'footers',\n",
    "                                   'quotes'))\n",
    "documents=dataset.data\n",
    "print(len(documents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 총 샘플 수는 11,314개\n",
    "# 전처리를 진행합니다.\n",
    "# 불필요한 토큰 제거하고, 정규화 합니다.\n",
    "\n",
    "news_df = pd.DataFrame({'document':documents})\n",
    "\n",
    "# 정규식\n",
    "news_df['clean_doc']  = news_df['document'].str.replace(\"[^a-zA-Z]\", \" \")\n",
    "\n",
    "# 길이가 3이하인 단어 제거\n",
    "news_df['clean_doc'] = news_df['clean_doc'].apply(lambda x: ' '.join([w for w in x.split() if len(w) > 3]))\n",
    "\n",
    "# 소문자  변환\n",
    "news_df['clean_doc'] = news_df['clean_doc'].apply(lambda x: x.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 결측값 확인\n",
    "\n",
    "news_df.isnull().values.any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 빈 값 유무 확인\n",
    "\n",
    "news_df.replace(\"\", float(\"NaN\"), inplace=True)\n",
    "news_df.isnull().values.any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10995\n"
     ]
    }
   ],
   "source": [
    "# 있네요.\n",
    "\n",
    "news_df.dropna(inplace=True)\n",
    "print(len(news_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 약 300개 나가리\n",
    "\n",
    "# 불용어 제거\n",
    "stop_words = stopwords.words('english')\n",
    "tokenized_doc = news_df['clean_doc'].apply(lambda x: x.split())\n",
    "tokenized_doc = tokenized_doc.apply(lambda x: [item for item in x if item not in stop_words])\n",
    "tokenized_doc = tokenized_doc.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10940\n"
     ]
    }
   ],
   "source": [
    "# 단어가 한 개 이하인 경우 찾아서 제거\n",
    "\n",
    "drop_train = [index for index, sentence in enumerate(tokenized_doc) if len(sentence) <= 1]\n",
    "tokenized_doc = np.delete(tokenized_doc, drop_train, axis=0)\n",
    "print(len(tokenized_doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 단어 집합을 생성하고 정수 인코딩\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(tokenized_doc)\n",
    "\n",
    "word2idx = tokenizer.word_index\n",
    "idx2word = {v:k for k, v in word2idx.items()}\n",
    "encoded = tokenizer.texts_to_sequences(tokenized_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[9, 59, 603, 207, 3278, 1495, 474, 702, 9470, 13686, 5533, 15227, 702, 442, 702, 70, 1148, 1095, 1036, 20294, 984, 705, 4294, 702, 217, 207, 1979, 15228, 13686, 4865, 4520, 87, 1530, 6, 52, 149, 581, 661, 4406, 4988, 4866, 1920, 755, 10668, 1102, 7837, 442, 957, 10669, 634, 51, 228, 2669, 4989, 178, 66, 222, 4521, 6066, 68, 4295], [1026, 532, 2, 60, 98, 582, 107, 800, 23, 79, 4522, 333, 7838, 864, 421, 3825, 458, 6488, 458, 2700, 4730, 333, 23, 9, 4731, 7262, 186, 310, 146, 170, 642, 1260, 107, 33568, 13, 985, 33569, 33570, 9471, 11491]]\n"
     ]
    }
   ],
   "source": [
    "print(encoded[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64277\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(word2idx) + 1\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.4.3. 네거티브 샘플링을 통한 데이터셋 구성하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "토큰화, 정제, 정규화, 불용어 제거, 정수 인코딩까지 일반적인 전처리 과정을 거쳤습니다.\n",
    "\n",
    "네거티브 샘플링을 통한 데이터셋을 구성할 차례입니다. 케라스의 skipgrams를 사용합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import skipgrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "skip_grams = [skipgrams(sample,\n",
    "                        vocabulary_size=vocab_size,\n",
    "                       window_size=10) for sample in encoded[:10]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(away (178), equality (11862)) -> 0\n",
      "(media (702), dowry (36585)) -> 0\n",
      "(degree (1530), shame (4988)) -> 1\n",
      "(seem (207), burzynski (10009)) -> 0\n",
      "(world (70), eaven (48964)) -> 0\n"
     ]
    }
   ],
   "source": [
    "# 첫 번째 샘플인 skip_grams[0] 내 skipgrams로 형성된 데이터셋 확인\n",
    "pairs, labels = skip_grams[0][0], skip_grams[0][1]\n",
    "for i in range(5):\n",
    "    print(\"({:s} ({:d}), {:s} ({:d})) -> {:d}\".format(idx2word[pairs[i][0]],\n",
    "                                                      pairs[i][0],\n",
    "                                                      idx2word[pairs[i][1]],\n",
    "                                                      pairs[i][1],\n",
    "                                                      labels[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "윈도우 크기 내에서 중심 단어, 주변 단어의 관계를 가지는 경우에는 1의 레이블을 갖도록 하고, 그렇지 않은 경우에는 0의 레이블을 가지도록 데이터셋을 구성합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 샘플 수 : 10\n"
     ]
    }
   ],
   "source": [
    "print('전체 샘플 수 :',len(skip_grams))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "encoded 중 상위 10개의 뉴스그룹 샘플에 대해서만 수행하였으므로 10이 출력됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2220\n",
      "2220\n"
     ]
    }
   ],
   "source": [
    "# 첫번째 뉴스그룹 샘플에 대해서 생긴 pairs와 labels의 개수\n",
    "print(len(pairs))\n",
    "print(len(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이 작업을 모든 뉴스그룹 샘플에 대해서 수행합니다.\n",
    "\n",
    "skip_grams = [skipgrams(sample,\n",
    "                        vocabulary_size=vocab_size,\n",
    "                        window_size=10) for sample in encoded]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.4.4. SGNS(Skip-Gram with Negative Sampling) 구현"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Skip-Gram을 직접 구현해봅니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Embedding, Reshape, Activation, Input\n",
    "from keras.layers import Dot\n",
    "from keras.utils import plot_model\n",
    "from IPython.display import SVG\n",
    "import pydot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 임베딩 차원은 100으로 하겠습니다.\n",
    "\n",
    "embed_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델을 설계하겠습니다. 우선, 두 개의 임베딩 테이블을 생성합니다.\n",
    "\n",
    "# 중심 단어를 위한 임베딩 테이블\n",
    "w_inputs = Input(shape=(1, ), dtype='int32')\n",
    "word_embedding = Embedding(vocab_size, embed_size)(w_inputs)\n",
    "\n",
    "# 주변 단어를 위한 임베딩 테이블\n",
    "c_inputs = Input(shape=(1, ), dtype='int32')\n",
    "context_embedding = Embedding(vocab_size, embed_size)(c_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 각 임베딩 테이블은 중심 단어와 주변 단어 각각을 위한 임베딩 테이블이며\n",
    "# 각 단어는 임베딩 테이블을 거쳐서 내적을 수행\n",
    "# 내적의 결과는 1 또는 0을 예측하기 위해서 시그모이드 함수를 활성화 함수로 거쳐 최종 예측 값 득\n",
    "\n",
    "dot_product = Dot(axes=2)([word_embedding, context_embedding])\n",
    "dot_product = Reshape((1, ), input_shape=(1, 1))(dot_product)\n",
    "output = Activation('sigmoid')(dot_product)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 1, 100)       6427700     input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, 1, 100)       6427700     input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dot_1 (Dot)                     (None, 1, 1)         0           embedding_1[0][0]                \n",
      "                                                                 embedding_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "reshape_1 (Reshape)             (None, 1)            0           dot_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 1)            0           reshape_1[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 12,855,400\n",
      "Trainable params: 12,855,400\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Model(inputs=[w_inputs, c_inputs], outputs=output)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Failed to import `pydot`. Please install `pydot`. For example with `pip install pydot`.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-48-a5bcb38c9aca>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mplot_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mto_file\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'model3.png'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshow_shapes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshow_layer_names\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrankdir\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'TB'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\envs\\Wyatt37\\lib\\site-packages\\keras\\utils\\vis_utils.py\u001b[0m in \u001b[0;36mplot_model\u001b[1;34m(model, to_file, show_shapes, show_layer_names, rankdir, expand_nested, dpi)\u001b[0m\n\u001b[0;32m    238\u001b[0m     \"\"\"\n\u001b[0;32m    239\u001b[0m     dot = model_to_dot(model, show_shapes, show_layer_names, rankdir,\n\u001b[1;32m--> 240\u001b[1;33m                        expand_nested, dpi)\n\u001b[0m\u001b[0;32m    241\u001b[0m     \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mextension\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplitext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mto_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    242\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mextension\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\Wyatt37\\lib\\site-packages\\keras\\utils\\vis_utils.py\u001b[0m in \u001b[0;36mmodel_to_dot\u001b[1;34m(model, show_shapes, show_layer_names, rankdir, expand_nested, dpi, subgraph)\u001b[0m\n\u001b[0;32m     77\u001b[0m     \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSequential\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     78\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 79\u001b[1;33m     \u001b[0m_check_pydot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     80\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0msubgraph\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     81\u001b[0m         \u001b[0mdot\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpydot\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCluster\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstyle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'dashed'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgraph_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\Wyatt37\\lib\\site-packages\\keras\\utils\\vis_utils.py\u001b[0m in \u001b[0;36m_check_pydot\u001b[1;34m()\u001b[0m\n\u001b[0;32m     20\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mpydot\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m         raise ImportError(\n\u001b[1;32m---> 22\u001b[1;33m             \u001b[1;34m'Failed to import `pydot`. '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m             \u001b[1;34m'Please install `pydot`. '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m             'For example with `pip install pydot`.')\n",
      "\u001b[1;31mImportError\u001b[0m: Failed to import `pydot`. Please install `pydot`. For example with `pip install pydot`."
     ]
    }
   ],
   "source": [
    "plot_model(model, to_file='model3.png', show_shapes=True, show_layer_names=True, rankdir='TB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thinp\\anaconda3\\envs\\Wyatt37\\lib\\site-packages\\tensorflow\\python\\framework\\indexed_slices.py:434: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-49-25f9fb6876e6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mfirst_elem\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msecond_elem\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[0mY\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m         \u001b[0mloss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Epoch: '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Loss: '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\Wyatt37\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[1;34m(self, x, y, sample_weight, class_weight, reset_metrics)\u001b[0m\n\u001b[0;32m   1512\u001b[0m             \u001b[0mins\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1513\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1514\u001b[1;33m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1515\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1516\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\Wyatt37\\lib\\site-packages\\tensorflow\\python\\keras\\backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   3790\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3791\u001b[0m       \u001b[0mconverted_inputs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3792\u001b[1;33m     \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_graph_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mconverted_inputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3793\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3794\u001b[0m     \u001b[1;31m# EagerTensor.numpy() will often make a copy to ensure memory safety.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\Wyatt37\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1603\u001b[0m       \u001b[0mTypeError\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mFor\u001b[0m \u001b[0minvalid\u001b[0m \u001b[0mpositional\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mkeyword\u001b[0m \u001b[0margument\u001b[0m \u001b[0mcombinations\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1604\u001b[0m     \"\"\"\n\u001b[1;32m-> 1605\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1606\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1607\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\Wyatt37\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1643\u001b[0m       raise TypeError(\"Keyword arguments {} unknown. Expected {}.\".format(\n\u001b[0;32m   1644\u001b[0m           list(kwargs.keys()), list(self._arg_keywords)))\n\u001b[1;32m-> 1645\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_flat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1646\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1647\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\Wyatt37\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1744\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1745\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[1;32m-> 1746\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[0;32m   1747\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[0;32m   1748\u001b[0m         \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\Wyatt37\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    596\u001b[0m               \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    597\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 598\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    599\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    600\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[1;32m~\\anaconda3\\envs\\Wyatt37\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[1;32m---> 60\u001b[1;33m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(1, 6):\n",
    "    loss = 0\n",
    "    for _, elem in enumerate(skip_grams):\n",
    "        first_elem = np.array(list(zip(*elem[0]))[0], dtype='int32')\n",
    "        second_elem = np.array(list(zip(*elem[0]))[1], dtype='int32')\n",
    "        labels = np.array(elem[1], dtype='int32')\n",
    "        X = [first_elem, second_elem]\n",
    "        Y = labels\n",
    "        loss += model.train_on_batch(X, Y)\n",
    "    print('Epoch: ', epoch, 'Loss: ', loss)\n",
    "    \n",
    "    # 너무 오래 걸려요."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10.5.  GloVe( 글로브)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "글로브(Global Vectors for Word Representation, GloVe)는 카운트 기반과 예측 기반을 모두 사용하는 방법론으로 2014년에 미국 스탠포드대학에서 개발한 단어 임베딩 방법론이다. 카운트 기반의 LSA와 예측 기반의 Word2Vec의 단점을 보완한 방법론으로 실제로 Word2Vec만큼 뛰어난 성능을 보인다. GloVe와 Word2Vec의 성능은 비등비등하며 비교사용하면 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이론은 이러하다. LSA가 카운트 기반으로 전체적인 통계 정보를 고려하기는 하지만, 단어 의미의 유추 작업에는 성능이 떨어진다. 반면 Word2Vec은 예측 기반이기 때문에 단어 간 유추 작업에는 뛰어나지만, 임베딩 벡터가 윈도우 크기 내에서만 주변 단어를 고려하기 때문에 코퍼스의 전체적인 통계 정보를 반영하지 못한다. 글로브는 이러한 두 방법론의 단점을 보완하기 위해 두 방법론을 모두 사용한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "설치가 안돼요...1시간 날렸네.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10.6. 사전 훈련된 워드 임베딩(Pre-trained Word Embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "요번 챕터에서는 케라스의 임베딩 층과 사전 훈련된 워드 임베딩을 가져와서 사용하는 것을 비교해봅니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.6.1. 케라스 임베딩 층(Keras Embedding Layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "케라스의 임베딩층은 입력 정수에 대해 밀집 벡터로 맵핑하고 이 밀집 벡터는 인공 신경망의 학습 과정에서 가중치가 학습되는 것과 같은 방식으로 훈련됩니다.\n",
    "\n",
    "임베딩 층은 다음과 같은 세 개의 인자를 받습니다.\n",
    "\n",
    "- vocab_size: 텍스트 데이터의 전체 단어 집합의 크기\n",
    "- output_dim: 워드 임베딩 후의 임베딩 벡터의 차원\n",
    "- input_length: 입력 시퀀스의 길이"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 예시로 대충 문장과 레이블 데이터를 만듭니다. 긍정이 1 부정이 0입니다.\n",
    "\n",
    "sentences = ['nice great best amazing', 'stop lies',\n",
    "             'pitiful nerd', 'excellent work', 'supreme quality',\n",
    "             'bad', 'highly respectable']\n",
    "y_train = [1, 0, 0, 1, 1, 0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n"
     ]
    }
   ],
   "source": [
    "# 가장 먼저 토큰화를 시킵니다.\n",
    "\n",
    "t = Tokenizer()\n",
    "t.fit_on_texts(sentences)\n",
    "vocab_size = len(t.word_index) + 1\n",
    "\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 2, 3, 4], [5, 6], [7, 8], [9, 10], [11, 12], [13], [14, 15]]\n"
     ]
    }
   ],
   "source": [
    "# 문장에 대한 정수 인코딩을 실시합니다.\n",
    "\n",
    "X_encoded = t.texts_to_sequences(sentences)\n",
    "print(X_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "max_len = max(len(l) for l in X_encoded)\n",
    "print(max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1  2  3  4]\n",
      " [ 5  6  0  0]\n",
      " [ 7  8  0  0]\n",
      " [ 9 10  0  0]\n",
      " [11 12  0  0]\n",
      " [13  0  0  0]\n",
      " [14 15  0  0]]\n"
     ]
    }
   ],
   "source": [
    "# 문장의 길이가 동일하게 패딩을 해주어야 합니다.\n",
    "\n",
    "X_train = pad_sequences(X_encoded, maxlen=max_len, padding='post')\n",
    "y_train = np.array(y_train)\n",
    "print(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "전처리가 끝났습니다. 모델링 시작합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, Flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Embedding(vocab_size, 4, input_length=max_len)) # 4차원 벡터\n",
    "model.add(Flatten()) # 1차원으로 짜부\n",
    "model.add(Dense(1, activation='sigmoid')) # 1차원 인풋"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thinp\\anaconda3\\envs\\Wyatt37\\lib\\site-packages\\tensorflow\\python\\framework\\indexed_slices.py:434: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      " - 0s - loss: 0.6934 - acc: 0.4286\n",
      "Epoch 2/100\n",
      " - 0s - loss: 0.6918 - acc: 0.5714\n",
      "Epoch 3/100\n",
      " - 0s - loss: 0.6905 - acc: 0.5714\n",
      "Epoch 4/100\n",
      " - 0s - loss: 0.6889 - acc: 0.7143\n",
      "Epoch 5/100\n",
      " - 0s - loss: 0.6873 - acc: 0.7143\n",
      "Epoch 6/100\n",
      " - 0s - loss: 0.6858 - acc: 0.7143\n",
      "Epoch 7/100\n",
      " - 0s - loss: 0.6842 - acc: 0.7143\n",
      "Epoch 8/100\n",
      " - 0s - loss: 0.6827 - acc: 0.7143\n",
      "Epoch 9/100\n",
      " - 0s - loss: 0.6811 - acc: 0.7143\n",
      "Epoch 10/100\n",
      " - 0s - loss: 0.6795 - acc: 0.7143\n",
      "Epoch 11/100\n",
      " - 0s - loss: 0.6780 - acc: 0.8571\n",
      "Epoch 12/100\n",
      " - 0s - loss: 0.6764 - acc: 1.0000\n",
      "Epoch 13/100\n",
      " - 0s - loss: 0.6748 - acc: 1.0000\n",
      "Epoch 14/100\n",
      " - 0s - loss: 0.6732 - acc: 1.0000\n",
      "Epoch 15/100\n",
      " - 0s - loss: 0.6716 - acc: 1.0000\n",
      "Epoch 16/100\n",
      " - 0s - loss: 0.6700 - acc: 1.0000\n",
      "Epoch 17/100\n",
      " - 0s - loss: 0.6683 - acc: 1.0000\n",
      "Epoch 18/100\n",
      " - 0s - loss: 0.6667 - acc: 1.0000\n",
      "Epoch 19/100\n",
      " - 0s - loss: 0.6651 - acc: 1.0000\n",
      "Epoch 20/100\n",
      " - 0s - loss: 0.6634 - acc: 1.0000\n",
      "Epoch 21/100\n",
      " - 0s - loss: 0.6618 - acc: 1.0000\n",
      "Epoch 22/100\n",
      " - 0s - loss: 0.6601 - acc: 1.0000\n",
      "Epoch 23/100\n",
      " - 0s - loss: 0.6584 - acc: 1.0000\n",
      "Epoch 24/100\n",
      " - 0s - loss: 0.6568 - acc: 1.0000\n",
      "Epoch 25/100\n",
      " - 0s - loss: 0.6551 - acc: 1.0000\n",
      "Epoch 26/100\n",
      " - 0s - loss: 0.6534 - acc: 1.0000\n",
      "Epoch 27/100\n",
      " - 0s - loss: 0.6517 - acc: 1.0000\n",
      "Epoch 28/100\n",
      " - 0s - loss: 0.6500 - acc: 1.0000\n",
      "Epoch 29/100\n",
      " - 0s - loss: 0.6483 - acc: 1.0000\n",
      "Epoch 30/100\n",
      " - 0s - loss: 0.6466 - acc: 1.0000\n",
      "Epoch 31/100\n",
      " - 0s - loss: 0.6449 - acc: 1.0000\n",
      "Epoch 32/100\n",
      " - 0s - loss: 0.6432 - acc: 1.0000\n",
      "Epoch 33/100\n",
      " - 0s - loss: 0.6415 - acc: 1.0000\n",
      "Epoch 34/100\n",
      " - 0s - loss: 0.6397 - acc: 1.0000\n",
      "Epoch 35/100\n",
      " - 0s - loss: 0.6380 - acc: 1.0000\n",
      "Epoch 36/100\n",
      " - 0s - loss: 0.6362 - acc: 1.0000\n",
      "Epoch 37/100\n",
      " - 0s - loss: 0.6344 - acc: 1.0000\n",
      "Epoch 38/100\n",
      " - 0s - loss: 0.6326 - acc: 1.0000\n",
      "Epoch 39/100\n",
      " - 0s - loss: 0.6309 - acc: 1.0000\n",
      "Epoch 40/100\n",
      " - 0s - loss: 0.6291 - acc: 1.0000\n",
      "Epoch 41/100\n",
      " - 0s - loss: 0.6273 - acc: 1.0000\n",
      "Epoch 42/100\n",
      " - 0s - loss: 0.6255 - acc: 1.0000\n",
      "Epoch 43/100\n",
      " - 0s - loss: 0.6236 - acc: 1.0000\n",
      "Epoch 44/100\n",
      " - 0s - loss: 0.6218 - acc: 1.0000\n",
      "Epoch 45/100\n",
      " - 0s - loss: 0.6200 - acc: 1.0000\n",
      "Epoch 46/100\n",
      " - 0s - loss: 0.6181 - acc: 1.0000\n",
      "Epoch 47/100\n",
      " - 0s - loss: 0.6163 - acc: 1.0000\n",
      "Epoch 48/100\n",
      " - 0s - loss: 0.6144 - acc: 1.0000\n",
      "Epoch 49/100\n",
      " - 0s - loss: 0.6125 - acc: 1.0000\n",
      "Epoch 50/100\n",
      " - 0s - loss: 0.6107 - acc: 1.0000\n",
      "Epoch 51/100\n",
      " - 0s - loss: 0.6088 - acc: 1.0000\n",
      "Epoch 52/100\n",
      " - 0s - loss: 0.6069 - acc: 1.0000\n",
      "Epoch 53/100\n",
      " - 0s - loss: 0.6049 - acc: 1.0000\n",
      "Epoch 54/100\n",
      " - 0s - loss: 0.6030 - acc: 1.0000\n",
      "Epoch 55/100\n",
      " - 0s - loss: 0.6011 - acc: 1.0000\n",
      "Epoch 56/100\n",
      " - 0s - loss: 0.5992 - acc: 1.0000\n",
      "Epoch 57/100\n",
      " - 0s - loss: 0.5972 - acc: 1.0000\n",
      "Epoch 58/100\n",
      " - 0s - loss: 0.5953 - acc: 1.0000\n",
      "Epoch 59/100\n",
      " - 0s - loss: 0.5933 - acc: 1.0000\n",
      "Epoch 60/100\n",
      " - 0s - loss: 0.5914 - acc: 1.0000\n",
      "Epoch 61/100\n",
      " - 0s - loss: 0.5894 - acc: 1.0000\n",
      "Epoch 62/100\n",
      " - 0s - loss: 0.5874 - acc: 1.0000\n",
      "Epoch 63/100\n",
      " - 0s - loss: 0.5854 - acc: 1.0000\n",
      "Epoch 64/100\n",
      " - 0s - loss: 0.5835 - acc: 1.0000\n",
      "Epoch 65/100\n",
      " - 0s - loss: 0.5815 - acc: 1.0000\n",
      "Epoch 66/100\n",
      " - 0s - loss: 0.5795 - acc: 1.0000\n",
      "Epoch 67/100\n",
      " - 0s - loss: 0.5775 - acc: 1.0000\n",
      "Epoch 68/100\n",
      " - 0s - loss: 0.5754 - acc: 1.0000\n",
      "Epoch 69/100\n",
      " - 0s - loss: 0.5734 - acc: 1.0000\n",
      "Epoch 70/100\n",
      " - 0s - loss: 0.5714 - acc: 1.0000\n",
      "Epoch 71/100\n",
      " - 0s - loss: 0.5693 - acc: 1.0000\n",
      "Epoch 72/100\n",
      " - 0s - loss: 0.5673 - acc: 1.0000\n",
      "Epoch 73/100\n",
      " - 0s - loss: 0.5652 - acc: 1.0000\n",
      "Epoch 74/100\n",
      " - 0s - loss: 0.5631 - acc: 1.0000\n",
      "Epoch 75/100\n",
      " - 0s - loss: 0.5611 - acc: 1.0000\n",
      "Epoch 76/100\n",
      " - 0s - loss: 0.5590 - acc: 1.0000\n",
      "Epoch 77/100\n",
      " - 0s - loss: 0.5569 - acc: 1.0000\n",
      "Epoch 78/100\n",
      " - 0s - loss: 0.5549 - acc: 1.0000\n",
      "Epoch 79/100\n",
      " - 0s - loss: 0.5528 - acc: 1.0000\n",
      "Epoch 80/100\n",
      " - 0s - loss: 0.5507 - acc: 1.0000\n",
      "Epoch 81/100\n",
      " - 0s - loss: 0.5486 - acc: 1.0000\n",
      "Epoch 82/100\n",
      " - 0s - loss: 0.5465 - acc: 1.0000\n",
      "Epoch 83/100\n",
      " - 0s - loss: 0.5444 - acc: 1.0000\n",
      "Epoch 84/100\n",
      " - 0s - loss: 0.5422 - acc: 1.0000\n",
      "Epoch 85/100\n",
      " - 0s - loss: 0.5401 - acc: 1.0000\n",
      "Epoch 86/100\n",
      " - 0s - loss: 0.5380 - acc: 1.0000\n",
      "Epoch 87/100\n",
      " - 0s - loss: 0.5359 - acc: 1.0000\n",
      "Epoch 88/100\n",
      " - 0s - loss: 0.5338 - acc: 1.0000\n",
      "Epoch 89/100\n",
      " - 0s - loss: 0.5316 - acc: 1.0000\n",
      "Epoch 90/100\n",
      " - 0s - loss: 0.5295 - acc: 1.0000\n",
      "Epoch 91/100\n",
      " - 0s - loss: 0.5274 - acc: 1.0000\n",
      "Epoch 92/100\n",
      " - 0s - loss: 0.5252 - acc: 1.0000\n",
      "Epoch 93/100\n",
      " - 0s - loss: 0.5231 - acc: 1.0000\n",
      "Epoch 94/100\n",
      " - 0s - loss: 0.5209 - acc: 1.0000\n",
      "Epoch 95/100\n",
      " - 0s - loss: 0.5187 - acc: 1.0000\n",
      "Epoch 96/100\n",
      " - 0s - loss: 0.5166 - acc: 1.0000\n",
      "Epoch 97/100\n",
      " - 0s - loss: 0.5144 - acc: 1.0000\n",
      "Epoch 98/100\n",
      " - 0s - loss: 0.5123 - acc: 1.0000\n",
      "Epoch 99/100\n",
      " - 0s - loss: 0.5101 - acc: 1.0000\n",
      "Epoch 100/100\n",
      " - 0s - loss: 0.5079 - acc: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x231fa152e48>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, epochs=100, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "뭐 대충 요렇게 돌아간다 라고 보시면 됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.6.2. 사전 훈련된 워드 임베딩(Pre-Trained Word Embedding) 사용하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "임베딩 벡터를 얻기 위해서 케라스의 Embedding()을 사용하기도 하지만, 때로는 이미 훈련되어져 있는 워드 임베딩을 불러서 이를 임베딩 벡터로 사용하기도 합니다.\n",
    "\n",
    "훈련 데이터가 적은 상황이라면 모델에 케라스의 Embedding()을 사용하는 것보다 다른 텍스트 데이터로 사전 훈련되어 있는 임베딩 벡터를 불러오는 것이 나은 선택일 수 이씃ㅂ니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- GloVe 다운로드 링크 : http://nlp.stanford.edu/data/glove.6B.zip\n",
    "- Word2Vec 다운로드 링크 : https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "훈련 데이터는 앞서 사용하였던 데이터를 쓰겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1  2  3  4]\n",
      " [ 5  6  0  0]\n",
      " [ 7  8  0  0]\n",
      " [ 9 10  0  0]\n",
      " [11 12  0  0]\n",
      " [13  0  0  0]\n",
      " [14 15  0  0]]\n"
     ]
    }
   ],
   "source": [
    "print(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 0 1 1 0 1]\n"
     ]
    }
   ],
   "source": [
    "print(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.6.2.1. 사전 훈련된 Word2Vec 사용하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thinp\\anaconda3\\envs\\Wyatt37\\lib\\importlib\\_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject\n",
      "  return f(*args, **kwds)\n",
      "C:\\Users\\thinp\\anaconda3\\envs\\Wyatt37\\lib\\importlib\\_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 구글의 사전 훈련된 Word2Vec 모델 로드\n",
    "\n",
    "word2vec_model = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin.gz', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3000000, 300)\n"
     ]
    }
   ],
   "source": [
    "print(word2vec_model.vectors.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "300차원을 가진 300만개의 벡터가 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16, 300)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 단어가 16개 이므로 16 x 300 의 영행렬을 만들어줍니다.\n",
    "\n",
    "embedding_matrix  = np.zeros((vocab_size, 300))\n",
    "np.shape(embedding_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word2vec_model 에서 특정 단어를 입력하면 해당 단어의 임베딩 벡터를 리턴 받고,\n",
    "# 만약 word2vec_model에 임베딩 벡터가 없다면 None을 리턴합니다.\n",
    "\n",
    "def get_vector(word):\n",
    "    if word in word2vec_model:\n",
    "        return word2vec_model[word]\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word, i in t.word_index.items(): # .items()는 dic의 키밸류를 리스트화\n",
    "    temp = get_vector(word)\n",
    "    if temp is not None:\n",
    "        embedding_matrix[i] = temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.15820312  0.10595703 -0.18945312  0.38671875  0.08349609 -0.26757812\n",
      "  0.08349609  0.11328125 -0.10400391  0.17871094 -0.12353516 -0.22265625\n",
      " -0.01806641 -0.25390625  0.13183594  0.0859375   0.16113281  0.11083984\n",
      " -0.11083984 -0.0859375   0.0267334   0.34570312  0.15136719 -0.00415039\n",
      "  0.10498047  0.04907227 -0.06982422  0.08642578  0.03198242 -0.02844238\n",
      " -0.15722656  0.11865234  0.36132812  0.00173187  0.05297852 -0.234375\n",
      "  0.11767578  0.08642578 -0.01123047  0.25976562  0.28515625 -0.11669922\n",
      "  0.38476562  0.07275391  0.01147461  0.03466797  0.18164062 -0.03955078\n",
      "  0.04199219  0.01013184 -0.06054688  0.09765625  0.06689453  0.14648438\n",
      " -0.12011719  0.08447266 -0.06152344  0.06347656  0.3046875  -0.35546875\n",
      " -0.2890625   0.19628906 -0.33203125 -0.07128906  0.12792969  0.09619141\n",
      " -0.12158203 -0.08691406 -0.12890625  0.27734375  0.265625    0.1796875\n",
      "  0.12695312  0.06298828 -0.34375    -0.05908203  0.0456543   0.171875\n",
      "  0.08935547  0.14648438 -0.04638672 -0.00842285 -0.0279541   0.234375\n",
      " -0.07470703 -0.13574219  0.00378418  0.19433594  0.05664062 -0.05419922\n",
      "  0.06176758  0.14160156 -0.24121094  0.02539062 -0.15917969 -0.10595703\n",
      "  0.11865234  0.24707031 -0.13574219 -0.20410156 -0.30078125  0.07910156\n",
      " -0.04394531  0.02026367 -0.05786133  0.2109375   0.13574219  0.08349609\n",
      " -0.0098877  -0.10546875 -0.08105469  0.03735352 -0.10351562 -0.10205078\n",
      "  0.23925781 -0.21875     0.05151367  0.06738281  0.07617188  0.04638672\n",
      "  0.03198242 -0.07275391  0.14550781  0.04858398 -0.05664062 -0.07470703\n",
      " -0.0030365  -0.09277344 -0.11083984 -0.03320312 -0.15234375 -0.12207031\n",
      "  0.09814453  0.375       0.00454712 -0.10009766  0.02734375  0.30078125\n",
      " -0.0390625   0.30078125 -0.04541016 -0.00424194  0.13671875 -0.18945312\n",
      " -0.21777344  0.12695312 -0.02746582 -0.18164062  0.08984375 -0.23339844\n",
      "  0.203125    0.2734375  -0.26953125  0.15332031 -0.20703125 -0.01153564\n",
      "  0.12451172  0.05395508 -0.23535156 -0.01409912 -0.09765625  0.20800781\n",
      "  0.19335938  0.14746094  0.28710938 -0.23046875  0.01965332 -0.09619141\n",
      " -0.0703125  -0.04174805 -0.17578125  0.0007019   0.10546875  0.10351562\n",
      "  0.02478027  0.35742188  0.17382812 -0.09570312 -0.18359375  0.23242188\n",
      " -0.14453125 -0.20410156 -0.01867676  0.06640625 -0.2265625  -0.00582886\n",
      " -0.08642578  0.02416992 -0.07324219 -0.29882812 -0.15625     0.07666016\n",
      "  0.19628906 -0.20410156  0.09863281 -0.01672363 -0.18652344 -0.12353516\n",
      " -0.16015625 -0.10058594  0.21777344  0.09375    -0.10058594 -0.03637695\n",
      "  0.15136719 -0.02526855 -0.23730469  0.03417969 -0.00604248  0.15625\n",
      " -0.14257812  0.18066406 -0.35351562  0.25        0.13085938 -0.04296875\n",
      "  0.17089844  0.20507812  0.00680542 -0.08251953 -0.06738281  0.22167969\n",
      " -0.16308594 -0.16699219 -0.02087402  0.11035156  0.06054688 -0.04223633\n",
      " -0.17285156  0.05029297 -0.19824219  0.01495361  0.06542969  0.03271484\n",
      "  0.14453125 -0.08691406 -0.11035156 -0.1484375   0.09667969  0.22363281\n",
      "  0.23535156  0.08398438  0.18164062 -0.10595703 -0.04296875  0.11572266\n",
      " -0.00153351  0.0534668  -0.1328125  -0.33203125 -0.08251953  0.30664062\n",
      "  0.22363281  0.27929688  0.09082031 -0.18066406 -0.00613403 -0.09423828\n",
      " -0.21289062  0.01965332 -0.08105469 -0.06689453 -0.31835938 -0.08447266\n",
      "  0.13574219  0.0625      0.07080078 -0.14257812 -0.11279297  0.01452637\n",
      " -0.06689453  0.03881836  0.19433594  0.09521484  0.11376953 -0.12451172\n",
      "  0.13769531 -0.18847656 -0.05224609  0.15820312  0.09863281 -0.04370117\n",
      " -0.06054688  0.21679688  0.04077148 -0.14648438 -0.18945312 -0.25195312\n",
      " -0.16894531 -0.08642578 -0.08544922  0.18945312 -0.14648438  0.13476562\n",
      " -0.04077148  0.03271484  0.08935547 -0.26757812  0.00836182 -0.21386719]\n"
     ]
    }
   ],
   "source": [
    "print(word2vec_model['nice'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어 nice의 정수 인덱스 : 1\n"
     ]
    }
   ],
   "source": [
    "print('단어 nice의 정수 인덱스 :', t.word_index['nice'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.15820312  0.10595703 -0.18945312  0.38671875  0.08349609 -0.26757812\n",
      "  0.08349609  0.11328125 -0.10400391  0.17871094 -0.12353516 -0.22265625\n",
      " -0.01806641 -0.25390625  0.13183594  0.0859375   0.16113281  0.11083984\n",
      " -0.11083984 -0.0859375   0.0267334   0.34570312  0.15136719 -0.00415039\n",
      "  0.10498047  0.04907227 -0.06982422  0.08642578  0.03198242 -0.02844238\n",
      " -0.15722656  0.11865234  0.36132812  0.00173187  0.05297852 -0.234375\n",
      "  0.11767578  0.08642578 -0.01123047  0.25976562  0.28515625 -0.11669922\n",
      "  0.38476562  0.07275391  0.01147461  0.03466797  0.18164062 -0.03955078\n",
      "  0.04199219  0.01013184 -0.06054688  0.09765625  0.06689453  0.14648438\n",
      " -0.12011719  0.08447266 -0.06152344  0.06347656  0.3046875  -0.35546875\n",
      " -0.2890625   0.19628906 -0.33203125 -0.07128906  0.12792969  0.09619141\n",
      " -0.12158203 -0.08691406 -0.12890625  0.27734375  0.265625    0.1796875\n",
      "  0.12695312  0.06298828 -0.34375    -0.05908203  0.0456543   0.171875\n",
      "  0.08935547  0.14648438 -0.04638672 -0.00842285 -0.0279541   0.234375\n",
      " -0.07470703 -0.13574219  0.00378418  0.19433594  0.05664062 -0.05419922\n",
      "  0.06176758  0.14160156 -0.24121094  0.02539062 -0.15917969 -0.10595703\n",
      "  0.11865234  0.24707031 -0.13574219 -0.20410156 -0.30078125  0.07910156\n",
      " -0.04394531  0.02026367 -0.05786133  0.2109375   0.13574219  0.08349609\n",
      " -0.0098877  -0.10546875 -0.08105469  0.03735352 -0.10351562 -0.10205078\n",
      "  0.23925781 -0.21875     0.05151367  0.06738281  0.07617188  0.04638672\n",
      "  0.03198242 -0.07275391  0.14550781  0.04858398 -0.05664062 -0.07470703\n",
      " -0.0030365  -0.09277344 -0.11083984 -0.03320312 -0.15234375 -0.12207031\n",
      "  0.09814453  0.375       0.00454712 -0.10009766  0.02734375  0.30078125\n",
      " -0.0390625   0.30078125 -0.04541016 -0.00424194  0.13671875 -0.18945312\n",
      " -0.21777344  0.12695312 -0.02746582 -0.18164062  0.08984375 -0.23339844\n",
      "  0.203125    0.2734375  -0.26953125  0.15332031 -0.20703125 -0.01153564\n",
      "  0.12451172  0.05395508 -0.23535156 -0.01409912 -0.09765625  0.20800781\n",
      "  0.19335938  0.14746094  0.28710938 -0.23046875  0.01965332 -0.09619141\n",
      " -0.0703125  -0.04174805 -0.17578125  0.0007019   0.10546875  0.10351562\n",
      "  0.02478027  0.35742188  0.17382812 -0.09570312 -0.18359375  0.23242188\n",
      " -0.14453125 -0.20410156 -0.01867676  0.06640625 -0.2265625  -0.00582886\n",
      " -0.08642578  0.02416992 -0.07324219 -0.29882812 -0.15625     0.07666016\n",
      "  0.19628906 -0.20410156  0.09863281 -0.01672363 -0.18652344 -0.12353516\n",
      " -0.16015625 -0.10058594  0.21777344  0.09375    -0.10058594 -0.03637695\n",
      "  0.15136719 -0.02526855 -0.23730469  0.03417969 -0.00604248  0.15625\n",
      " -0.14257812  0.18066406 -0.35351562  0.25        0.13085938 -0.04296875\n",
      "  0.17089844  0.20507812  0.00680542 -0.08251953 -0.06738281  0.22167969\n",
      " -0.16308594 -0.16699219 -0.02087402  0.11035156  0.06054688 -0.04223633\n",
      " -0.17285156  0.05029297 -0.19824219  0.01495361  0.06542969  0.03271484\n",
      "  0.14453125 -0.08691406 -0.11035156 -0.1484375   0.09667969  0.22363281\n",
      "  0.23535156  0.08398438  0.18164062 -0.10595703 -0.04296875  0.11572266\n",
      " -0.00153351  0.0534668  -0.1328125  -0.33203125 -0.08251953  0.30664062\n",
      "  0.22363281  0.27929688  0.09082031 -0.18066406 -0.00613403 -0.09423828\n",
      " -0.21289062  0.01965332 -0.08105469 -0.06689453 -0.31835938 -0.08447266\n",
      "  0.13574219  0.0625      0.07080078 -0.14257812 -0.11279297  0.01452637\n",
      " -0.06689453  0.03881836  0.19433594  0.09521484  0.11376953 -0.12451172\n",
      "  0.13769531 -0.18847656 -0.05224609  0.15820312  0.09863281 -0.04370117\n",
      " -0.06054688  0.21679688  0.04077148 -0.14648438 -0.18945312 -0.25195312\n",
      " -0.16894531 -0.08642578 -0.08544922  0.18945312 -0.14648438  0.13476562\n",
      " -0.04077148  0.03271484  0.08935547 -0.26757812  0.00836182 -0.21386719]\n"
     ]
    }
   ],
   "source": [
    "print(embedding_matrix[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "두 임베딩 벡터 값이 동일하므로 잘 맵핑된 것을 알 수 있습니다. 이제 학습을 하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "e = Embedding(vocab_size,\n",
    "              300,\n",
    "              weights=[embedding_matrix],\n",
    "              input_length=max_len,\n",
    "              trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(e)\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "             loss='binary_crossentropy',\n",
    "             metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      " - 0s - loss: 0.7169 - acc: 0.2857\n",
      "Epoch 2/100\n",
      " - 0s - loss: 0.6977 - acc: 0.2857\n",
      "Epoch 3/100\n",
      " - 0s - loss: 0.6790 - acc: 0.5714\n",
      "Epoch 4/100\n",
      " - 0s - loss: 0.6610 - acc: 0.5714\n",
      "Epoch 5/100\n",
      " - 0s - loss: 0.6434 - acc: 0.8571\n",
      "Epoch 6/100\n",
      " - 0s - loss: 0.6265 - acc: 0.8571\n",
      "Epoch 7/100\n",
      " - 0s - loss: 0.6101 - acc: 1.0000\n",
      "Epoch 8/100\n",
      " - 0s - loss: 0.5942 - acc: 1.0000\n",
      "Epoch 9/100\n",
      " - 0s - loss: 0.5791 - acc: 1.0000\n",
      "Epoch 10/100\n",
      " - 0s - loss: 0.5643 - acc: 1.0000\n",
      "Epoch 11/100\n",
      " - 0s - loss: 0.5501 - acc: 1.0000\n",
      "Epoch 12/100\n",
      " - 0s - loss: 0.5364 - acc: 1.0000\n",
      "Epoch 13/100\n",
      " - 0s - loss: 0.5230 - acc: 1.0000\n",
      "Epoch 14/100\n",
      " - 0s - loss: 0.5102 - acc: 1.0000\n",
      "Epoch 15/100\n",
      " - 0s - loss: 0.4977 - acc: 1.0000\n",
      "Epoch 16/100\n",
      " - 0s - loss: 0.4858 - acc: 1.0000\n",
      "Epoch 17/100\n",
      " - 0s - loss: 0.4742 - acc: 1.0000\n",
      "Epoch 18/100\n",
      " - 0s - loss: 0.4631 - acc: 1.0000\n",
      "Epoch 19/100\n",
      " - 0s - loss: 0.4523 - acc: 1.0000\n",
      "Epoch 20/100\n",
      " - 0s - loss: 0.4418 - acc: 1.0000\n",
      "Epoch 21/100\n",
      " - 0s - loss: 0.4317 - acc: 1.0000\n",
      "Epoch 22/100\n",
      " - 0s - loss: 0.4219 - acc: 1.0000\n",
      "Epoch 23/100\n",
      " - 0s - loss: 0.4124 - acc: 1.0000\n",
      "Epoch 24/100\n",
      " - 0s - loss: 0.4033 - acc: 1.0000\n",
      "Epoch 25/100\n",
      " - 0s - loss: 0.3944 - acc: 1.0000\n",
      "Epoch 26/100\n",
      " - 0s - loss: 0.3858 - acc: 1.0000\n",
      "Epoch 27/100\n",
      " - 0s - loss: 0.3774 - acc: 1.0000\n",
      "Epoch 28/100\n",
      " - 0s - loss: 0.3693 - acc: 1.0000\n",
      "Epoch 29/100\n",
      " - 0s - loss: 0.3613 - acc: 1.0000\n",
      "Epoch 30/100\n",
      " - 0s - loss: 0.3537 - acc: 1.0000\n",
      "Epoch 31/100\n",
      " - 0s - loss: 0.3463 - acc: 1.0000\n",
      "Epoch 32/100\n",
      " - 0s - loss: 0.3391 - acc: 1.0000\n",
      "Epoch 33/100\n",
      " - 0s - loss: 0.3322 - acc: 1.0000\n",
      "Epoch 34/100\n",
      " - 0s - loss: 0.3254 - acc: 1.0000\n",
      "Epoch 35/100\n",
      " - 0s - loss: 0.3188 - acc: 1.0000\n",
      "Epoch 36/100\n",
      " - 0s - loss: 0.3124 - acc: 1.0000\n",
      "Epoch 37/100\n",
      " - 0s - loss: 0.3062 - acc: 1.0000\n",
      "Epoch 38/100\n",
      " - 0s - loss: 0.3002 - acc: 1.0000\n",
      "Epoch 39/100\n",
      " - 0s - loss: 0.2943 - acc: 1.0000\n",
      "Epoch 40/100\n",
      " - 0s - loss: 0.2886 - acc: 1.0000\n",
      "Epoch 41/100\n",
      " - 0s - loss: 0.2830 - acc: 1.0000\n",
      "Epoch 42/100\n",
      " - 0s - loss: 0.2777 - acc: 1.0000\n",
      "Epoch 43/100\n",
      " - 0s - loss: 0.2724 - acc: 1.0000\n",
      "Epoch 44/100\n",
      " - 0s - loss: 0.2673 - acc: 1.0000\n",
      "Epoch 45/100\n",
      " - 0s - loss: 0.2624 - acc: 1.0000\n",
      "Epoch 46/100\n",
      " - 0s - loss: 0.2575 - acc: 1.0000\n",
      "Epoch 47/100\n",
      " - 0s - loss: 0.2528 - acc: 1.0000\n",
      "Epoch 48/100\n",
      " - 0s - loss: 0.2483 - acc: 1.0000\n",
      "Epoch 49/100\n",
      " - 0s - loss: 0.2438 - acc: 1.0000\n",
      "Epoch 50/100\n",
      " - 0s - loss: 0.2394 - acc: 1.0000\n",
      "Epoch 51/100\n",
      " - 0s - loss: 0.2352 - acc: 1.0000\n",
      "Epoch 52/100\n",
      " - 0s - loss: 0.2311 - acc: 1.0000\n",
      "Epoch 53/100\n",
      " - 0s - loss: 0.2271 - acc: 1.0000\n",
      "Epoch 54/100\n",
      " - 0s - loss: 0.2232 - acc: 1.0000\n",
      "Epoch 55/100\n",
      " - 0s - loss: 0.2194 - acc: 1.0000\n",
      "Epoch 56/100\n",
      " - 0s - loss: 0.2157 - acc: 1.0000\n",
      "Epoch 57/100\n",
      " - 0s - loss: 0.2121 - acc: 1.0000\n",
      "Epoch 58/100\n",
      " - 0s - loss: 0.2085 - acc: 1.0000\n",
      "Epoch 59/100\n",
      " - 0s - loss: 0.2051 - acc: 1.0000\n",
      "Epoch 60/100\n",
      " - 0s - loss: 0.2017 - acc: 1.0000\n",
      "Epoch 61/100\n",
      " - 0s - loss: 0.1985 - acc: 1.0000\n",
      "Epoch 62/100\n",
      " - 0s - loss: 0.1953 - acc: 1.0000\n",
      "Epoch 63/100\n",
      " - 0s - loss: 0.1922 - acc: 1.0000\n",
      "Epoch 64/100\n",
      " - 0s - loss: 0.1892 - acc: 1.0000\n",
      "Epoch 65/100\n",
      " - 0s - loss: 0.1862 - acc: 1.0000\n",
      "Epoch 66/100\n",
      " - 0s - loss: 0.1833 - acc: 1.0000\n",
      "Epoch 67/100\n",
      " - 0s - loss: 0.1805 - acc: 1.0000\n",
      "Epoch 68/100\n",
      " - 0s - loss: 0.1777 - acc: 1.0000\n",
      "Epoch 69/100\n",
      " - 0s - loss: 0.1750 - acc: 1.0000\n",
      "Epoch 70/100\n",
      " - 0s - loss: 0.1724 - acc: 1.0000\n",
      "Epoch 71/100\n",
      " - 0s - loss: 0.1699 - acc: 1.0000\n",
      "Epoch 72/100\n",
      " - 0s - loss: 0.1673 - acc: 1.0000\n",
      "Epoch 73/100\n",
      " - 0s - loss: 0.1649 - acc: 1.0000\n",
      "Epoch 74/100\n",
      " - 0s - loss: 0.1625 - acc: 1.0000\n",
      "Epoch 75/100\n",
      " - 0s - loss: 0.1601 - acc: 1.0000\n",
      "Epoch 76/100\n",
      " - 0s - loss: 0.1578 - acc: 1.0000\n",
      "Epoch 77/100\n",
      " - 0s - loss: 0.1556 - acc: 1.0000\n",
      "Epoch 78/100\n",
      " - 0s - loss: 0.1534 - acc: 1.0000\n",
      "Epoch 79/100\n",
      " - 0s - loss: 0.1512 - acc: 1.0000\n",
      "Epoch 80/100\n",
      " - 0s - loss: 0.1491 - acc: 1.0000\n",
      "Epoch 81/100\n",
      " - 0s - loss: 0.1471 - acc: 1.0000\n",
      "Epoch 82/100\n",
      " - 0s - loss: 0.1451 - acc: 1.0000\n",
      "Epoch 83/100\n",
      " - 0s - loss: 0.1431 - acc: 1.0000\n",
      "Epoch 84/100\n",
      " - 0s - loss: 0.1412 - acc: 1.0000\n",
      "Epoch 85/100\n",
      " - 0s - loss: 0.1393 - acc: 1.0000\n",
      "Epoch 86/100\n",
      " - 0s - loss: 0.1374 - acc: 1.0000\n",
      "Epoch 87/100\n",
      " - 0s - loss: 0.1356 - acc: 1.0000\n",
      "Epoch 88/100\n",
      " - 0s - loss: 0.1339 - acc: 1.0000\n",
      "Epoch 89/100\n",
      " - 0s - loss: 0.1321 - acc: 1.0000\n",
      "Epoch 90/100\n",
      " - 0s - loss: 0.1304 - acc: 1.0000\n",
      "Epoch 91/100\n",
      " - 0s - loss: 0.1288 - acc: 1.0000\n",
      "Epoch 92/100\n",
      " - 0s - loss: 0.1271 - acc: 1.0000\n",
      "Epoch 93/100\n",
      " - 0s - loss: 0.1255 - acc: 1.0000\n",
      "Epoch 94/100\n",
      " - 0s - loss: 0.1240 - acc: 1.0000\n",
      "Epoch 95/100\n",
      " - 0s - loss: 0.1224 - acc: 1.0000\n",
      "Epoch 96/100\n",
      " - 0s - loss: 0.1209 - acc: 1.0000\n",
      "Epoch 97/100\n",
      " - 0s - loss: 0.1194 - acc: 1.0000\n",
      "Epoch 98/100\n",
      " - 0s - loss: 0.1180 - acc: 1.0000\n",
      "Epoch 99/100\n",
      " - 0s - loss: 0.1166 - acc: 1.0000\n",
      "Epoch 100/100\n",
      " - 0s - loss: 0.1152 - acc: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x2319b2b0780>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, epochs=100, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.6.2.2. 사전 훈련된 GloVe 사용하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "우선 다운로드 받은 파일인 glove.6B.zip의 압축을 풀면 그 안에 4개의 파일이 있는데 여기서 사용할 파일은 glove.6B.100d.txt 파일입니다. 해당 파일은 하나의 줄당 101개의 값을 가지는 리스트를 갖고 있습니다. 두 개의 줄만 읽어보도록 하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  glove.6B.zip\n",
      "  inflating: glove.6B.50d.txt        \n",
      "  inflating: glove.6B.100d.txt       \n",
      "  inflating: glove.6B.200d.txt       \n",
      "  inflating: glove.6B.300d.txt       \n"
     ]
    }
   ],
   "source": [
    "!unzip glove*.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "n=0\n",
    "f = open('glove.6B.100d.txt', encoding=\"utf8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', '-0.038194', '-0.24487', '0.72812', '-0.39961', '0.083172', '0.043953', '-0.39141', '0.3344', '-0.57545', '0.087459', '0.28787', '-0.06731', '0.30906', '-0.26384', '-0.13231', '-0.20757', '0.33395', '-0.33848', '-0.31743', '-0.48336', '0.1464', '-0.37304', '0.34577', '0.052041', '0.44946', '-0.46971', '0.02628', '-0.54155', '-0.15518', '-0.14107', '-0.039722', '0.28277', '0.14393', '0.23464', '-0.31021', '0.086173', '0.20397', '0.52624', '0.17164', '-0.082378', '-0.71787', '-0.41531', '0.20335', '-0.12763', '0.41367', '0.55187', '0.57908', '-0.33477', '-0.36559', '-0.54857', '-0.062892', '0.26584', '0.30205', '0.99775', '-0.80481', '-3.0243', '0.01254', '-0.36942', '2.2167', '0.72201', '-0.24978', '0.92136', '0.034514', '0.46745', '1.1079', '-0.19358', '-0.074575', '0.23353', '-0.052062', '-0.22044', '0.057162', '-0.15806', '-0.30798', '-0.41625', '0.37972', '0.15006', '-0.53212', '-0.2055', '-1.2526', '0.071624', '0.70565', '0.49744', '-0.42063', '0.26148', '-1.538', '-0.30223', '-0.073438', '-0.28312', '0.37104', '-0.25217', '0.016215', '-0.017099', '-0.38984', '0.87424', '-0.72569', '-0.51058', '-0.52028', '-0.1459', '0.8278', '0.27062']\n",
      "the\n",
      "[',', '-0.10767', '0.11053', '0.59812', '-0.54361', '0.67396', '0.10663', '0.038867', '0.35481', '0.06351', '-0.094189', '0.15786', '-0.81665', '0.14172', '0.21939', '0.58505', '-0.52158', '0.22783', '-0.16642', '-0.68228', '0.3587', '0.42568', '0.19021', '0.91963', '0.57555', '0.46185', '0.42363', '-0.095399', '-0.42749', '-0.16567', '-0.056842', '-0.29595', '0.26037', '-0.26606', '-0.070404', '-0.27662', '0.15821', '0.69825', '0.43081', '0.27952', '-0.45437', '-0.33801', '-0.58184', '0.22364', '-0.5778', '-0.26862', '-0.20425', '0.56394', '-0.58524', '-0.14365', '-0.64218', '0.0054697', '-0.35248', '0.16162', '1.1796', '-0.47674', '-2.7553', '-0.1321', '-0.047729', '1.0655', '1.1034', '-0.2208', '0.18669', '0.13177', '0.15117', '0.7131', '-0.35215', '0.91348', '0.61783', '0.70992', '0.23955', '-0.14571', '-0.37859', '-0.045959', '-0.47368', '0.2385', '0.20536', '-0.18996', '0.32507', '-1.1112', '-0.36341', '0.98679', '-0.084776', '-0.54008', '0.11726', '-1.0194', '-0.24424', '0.12771', '0.013884', '0.080374', '-0.35414', '0.34951', '-0.7226', '0.37549', '0.4441', '-0.99059', '0.61214', '-0.35111', '-0.83155', '0.45293', '0.082577']\n",
      ",\n"
     ]
    }
   ],
   "source": [
    "for line in f:\n",
    "    word_vector = line.split() # 각 줄을 읽어와서 word_vector에 저장.\n",
    "    print(word_vector) # 각 줄을 출력\n",
    "    word = word_vector[0] # word_vector에서 첫 번째 값만 저장\n",
    "    print(word) # word_vector의 첫 번째 값만 출력\n",
    "    n+=1\n",
    "    if n==2:\n",
    "        break\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "101\n"
     ]
    }
   ],
   "source": [
    "print(type(word_vector))\n",
    "print(len(word_vector))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "101개의 값 중에서 첫번째 값은 임베딩 벡터가 의미하는 단어를 의미하며, 두번째 값부터 마지막 값은 해당 단어의 임베딩 벡터의 100개의 차원에서의 각 값을 의미합니다. 즉, glove.6B.100d.txt는 수많은 단어에 대해서 100개의 차원을 가지는 임베딩 벡터로 제공하고 있습니다. 위의 출력 결과는 단어 'the'에 대해서 100개의 차원을 가지는 임베딩 벡터와 단어 ','에 대해서 100개의 차원을 가지는 임베딩 벡터를 보여줍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400000\n"
     ]
    }
   ],
   "source": [
    "# 모든 값을 불러와봅니다.\n",
    "\n",
    "embedding_dict = dict()\n",
    "f = open('glove.6B.100d.txt', encoding=\"utf8\")\n",
    "\n",
    "for line in f:\n",
    "    word_vector = line.split()\n",
    "    word = word_vector[0]\n",
    "    word_vector_arr = np.asarray(word_vector[1:], dtype='float32')\n",
    "    # 100개의 값을 가지는 array로 변환\n",
    "    embedding_dict[word] = word_vector_arr\n",
    "    # 키, 밸류(어레이) 로 할당.\n",
    "\n",
    "f.close()\n",
    "print(len(embedding_dict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "샘플 가져와보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.049773   0.19903    0.10585    0.1391    -0.32395    0.44053\n",
      "  0.3947    -0.22805   -0.25793    0.49768    0.15384   -0.08831\n",
      "  0.0782    -0.8299    -0.037788   0.16772   -0.45197   -0.17085\n",
      "  0.74756    0.98256    0.81872    0.28507    0.16178   -0.48626\n",
      " -0.006265  -0.92469   -0.30625   -0.067318  -0.046762  -0.76291\n",
      " -0.0025264 -0.018795   0.12882   -0.52457    0.3586     0.43119\n",
      " -0.89477   -0.057421  -0.53724    0.25587    0.55195    0.44698\n",
      " -0.24252    0.29946    0.25776   -0.8717     0.68426   -0.05688\n",
      " -0.1848    -0.59352   -0.11227   -0.57692   -0.013593   0.18488\n",
      " -0.32507   -0.90171    0.17672    0.075601   0.54896   -0.21488\n",
      " -0.54018   -0.45882   -0.79536    0.26331    0.18879   -0.16363\n",
      "  0.3975     0.1099     0.1164    -0.083499   0.50159    0.35802\n",
      "  0.25677    0.088546   0.42108    0.28674   -0.71285   -0.82915\n",
      "  0.15297   -0.82712    0.022112   1.067     -0.31776    0.1211\n",
      " -0.069755  -0.61327    0.27308   -0.42638   -0.085084  -0.17694\n",
      " -0.0090944  0.1109     0.62543   -0.23682   -0.44928   -0.3667\n",
      " -0.21616   -0.19187   -0.032502   0.38025  ]\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "print(embedding_dict['respectable'])\n",
    "print(len(embedding_dict['respectable']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16, 100)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix = np.zeros((vocab_size, 100))\n",
    "np.shape(embedding_matrix)\n",
    "# 영행렬을 만들어줍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_items([('nice', 1), ('great', 2), ('best', 3), ('amazing', 4), ('stop', 5), ('lies', 6), ('pitiful', 7), ('nerd', 8), ('excellent', 9), ('work', 10), ('supreme', 11), ('quality', 12), ('bad', 13), ('highly', 14), ('respectable', 15)])\n"
     ]
    }
   ],
   "source": [
    "print(t.word_index.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 글로브 임베딩 벡터를 훈련데이터에 맵핑하겠습니다.\n",
    "\n",
    "for word, i in t.word_index.items(): # 단어 하나씩 꺼내옵니다.\n",
    "    temp = embedding_dict.get(word) # 임베딩 벡터 100개를 할당\n",
    "    if temp is not None:\n",
    "        embedding_matrix[i] = temp # 맵핑"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 이를 활용하여 임베딩 레이어를 만들겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "e = Embedding(vocab_size, # 전체 단어 사이즈.\n",
    "              100, # 임베딩 차원\n",
    "              weights=[embedding_matrix], # 가중치는 임베딩 벡터 값\n",
    "              input_length=max_len, # 문장 길이는 맥스 렌\n",
    "              trainable=False) # 기훈련이기에 trainable은 False로"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(e)\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "             loss='binary_crossentropy',\n",
    "             metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      " - 0s - loss: 0.8526 - acc: 0.4286\n",
      "Epoch 2/100\n",
      " - 0s - loss: 0.8310 - acc: 0.4286\n",
      "Epoch 3/100\n",
      " - 0s - loss: 0.8101 - acc: 0.4286\n",
      "Epoch 4/100\n",
      " - 0s - loss: 0.7896 - acc: 0.4286\n",
      "Epoch 5/100\n",
      " - 0s - loss: 0.7697 - acc: 0.4286\n",
      "Epoch 6/100\n",
      " - 0s - loss: 0.7503 - acc: 0.4286\n",
      "Epoch 7/100\n",
      " - 0s - loss: 0.7315 - acc: 0.4286\n",
      "Epoch 8/100\n",
      " - 0s - loss: 0.7131 - acc: 0.4286\n",
      "Epoch 9/100\n",
      " - 0s - loss: 0.6953 - acc: 0.5714\n",
      "Epoch 10/100\n",
      " - 0s - loss: 0.6779 - acc: 0.5714\n",
      "Epoch 11/100\n",
      " - 0s - loss: 0.6610 - acc: 0.5714\n",
      "Epoch 12/100\n",
      " - 0s - loss: 0.6446 - acc: 0.5714\n",
      "Epoch 13/100\n",
      " - 0s - loss: 0.6287 - acc: 0.5714\n",
      "Epoch 14/100\n",
      " - 0s - loss: 0.6131 - acc: 0.5714\n",
      "Epoch 15/100\n",
      " - 0s - loss: 0.5981 - acc: 0.5714\n",
      "Epoch 16/100\n",
      " - 0s - loss: 0.5835 - acc: 0.7143\n",
      "Epoch 17/100\n",
      " - 0s - loss: 0.5693 - acc: 0.7143\n",
      "Epoch 18/100\n",
      " - 0s - loss: 0.5555 - acc: 0.8571\n",
      "Epoch 19/100\n",
      " - 0s - loss: 0.5422 - acc: 0.8571\n",
      "Epoch 20/100\n",
      " - 0s - loss: 0.5293 - acc: 0.8571\n",
      "Epoch 21/100\n",
      " - 0s - loss: 0.5167 - acc: 0.8571\n",
      "Epoch 22/100\n",
      " - 0s - loss: 0.5045 - acc: 0.8571\n",
      "Epoch 23/100\n",
      " - 0s - loss: 0.4927 - acc: 0.8571\n",
      "Epoch 24/100\n",
      " - 0s - loss: 0.4811 - acc: 0.8571\n",
      "Epoch 25/100\n",
      " - 0s - loss: 0.4701 - acc: 1.0000\n",
      "Epoch 26/100\n",
      " - 0s - loss: 0.4592 - acc: 1.0000\n",
      "Epoch 27/100\n",
      " - 0s - loss: 0.4488 - acc: 1.0000\n",
      "Epoch 28/100\n",
      " - 0s - loss: 0.4387 - acc: 1.0000\n",
      "Epoch 29/100\n",
      " - 0s - loss: 0.4290 - acc: 1.0000\n",
      "Epoch 30/100\n",
      " - 0s - loss: 0.4195 - acc: 1.0000\n",
      "Epoch 31/100\n",
      " - 0s - loss: 0.4103 - acc: 1.0000\n",
      "Epoch 32/100\n",
      " - 0s - loss: 0.4014 - acc: 1.0000\n",
      "Epoch 33/100\n",
      " - 0s - loss: 0.3927 - acc: 1.0000\n",
      "Epoch 34/100\n",
      " - 0s - loss: 0.3843 - acc: 1.0000\n",
      "Epoch 35/100\n",
      " - 0s - loss: 0.3761 - acc: 1.0000\n",
      "Epoch 36/100\n",
      " - 0s - loss: 0.3681 - acc: 1.0000\n",
      "Epoch 37/100\n",
      " - 0s - loss: 0.3604 - acc: 1.0000\n",
      "Epoch 38/100\n",
      " - 0s - loss: 0.3529 - acc: 1.0000\n",
      "Epoch 39/100\n",
      " - 0s - loss: 0.3456 - acc: 1.0000\n",
      "Epoch 40/100\n",
      " - 0s - loss: 0.3385 - acc: 1.0000\n",
      "Epoch 41/100\n",
      " - 0s - loss: 0.3316 - acc: 1.0000\n",
      "Epoch 42/100\n",
      " - 0s - loss: 0.3249 - acc: 1.0000\n",
      "Epoch 43/100\n",
      " - 0s - loss: 0.3183 - acc: 1.0000\n",
      "Epoch 44/100\n",
      " - 0s - loss: 0.3120 - acc: 1.0000\n",
      "Epoch 45/100\n",
      " - 0s - loss: 0.3058 - acc: 1.0000\n",
      "Epoch 46/100\n",
      " - 0s - loss: 0.2998 - acc: 1.0000\n",
      "Epoch 47/100\n",
      " - 0s - loss: 0.2940 - acc: 1.0000\n",
      "Epoch 48/100\n",
      " - 0s - loss: 0.2883 - acc: 1.0000\n",
      "Epoch 49/100\n",
      " - 0s - loss: 0.2827 - acc: 1.0000\n",
      "Epoch 50/100\n",
      " - 0s - loss: 0.2773 - acc: 1.0000\n",
      "Epoch 51/100\n",
      " - 0s - loss: 0.2721 - acc: 1.0000\n",
      "Epoch 52/100\n",
      " - 0s - loss: 0.2670 - acc: 1.0000\n",
      "Epoch 53/100\n",
      " - 0s - loss: 0.2620 - acc: 1.0000\n",
      "Epoch 54/100\n",
      " - 0s - loss: 0.2571 - acc: 1.0000\n",
      "Epoch 55/100\n",
      " - 0s - loss: 0.2524 - acc: 1.0000\n",
      "Epoch 56/100\n",
      " - 0s - loss: 0.2478 - acc: 1.0000\n",
      "Epoch 57/100\n",
      " - 0s - loss: 0.2433 - acc: 1.0000\n",
      "Epoch 58/100\n",
      " - 0s - loss: 0.2389 - acc: 1.0000\n",
      "Epoch 59/100\n",
      " - 0s - loss: 0.2347 - acc: 1.0000\n",
      "Epoch 60/100\n",
      " - 0s - loss: 0.2305 - acc: 1.0000\n",
      "Epoch 61/100\n",
      " - 0s - loss: 0.2265 - acc: 1.0000\n",
      "Epoch 62/100\n",
      " - 0s - loss: 0.2225 - acc: 1.0000\n",
      "Epoch 63/100\n",
      " - 0s - loss: 0.2187 - acc: 1.0000\n",
      "Epoch 64/100\n",
      " - 0s - loss: 0.2150 - acc: 1.0000\n",
      "Epoch 65/100\n",
      " - 0s - loss: 0.2113 - acc: 1.0000\n",
      "Epoch 66/100\n",
      " - 0s - loss: 0.2078 - acc: 1.0000\n",
      "Epoch 67/100\n",
      " - 0s - loss: 0.2043 - acc: 1.0000\n",
      "Epoch 68/100\n",
      " - 0s - loss: 0.2009 - acc: 1.0000\n",
      "Epoch 69/100\n",
      " - 0s - loss: 0.1976 - acc: 1.0000\n",
      "Epoch 70/100\n",
      " - 0s - loss: 0.1944 - acc: 1.0000\n",
      "Epoch 71/100\n",
      " - 0s - loss: 0.1913 - acc: 1.0000\n",
      "Epoch 72/100\n",
      " - 0s - loss: 0.1882 - acc: 1.0000\n",
      "Epoch 73/100\n",
      " - 0s - loss: 0.1852 - acc: 1.0000\n",
      "Epoch 74/100\n",
      " - 0s - loss: 0.1823 - acc: 1.0000\n",
      "Epoch 75/100\n",
      " - 0s - loss: 0.1795 - acc: 1.0000\n",
      "Epoch 76/100\n",
      " - 0s - loss: 0.1767 - acc: 1.0000\n",
      "Epoch 77/100\n",
      " - 0s - loss: 0.1740 - acc: 1.0000\n",
      "Epoch 78/100\n",
      " - 0s - loss: 0.1713 - acc: 1.0000\n",
      "Epoch 79/100\n",
      " - 0s - loss: 0.1688 - acc: 1.0000\n",
      "Epoch 80/100\n",
      " - 0s - loss: 0.1662 - acc: 1.0000\n",
      "Epoch 81/100\n",
      " - 0s - loss: 0.1638 - acc: 1.0000\n",
      "Epoch 82/100\n",
      " - 0s - loss: 0.1614 - acc: 1.0000\n",
      "Epoch 83/100\n",
      " - 0s - loss: 0.1590 - acc: 1.0000\n",
      "Epoch 84/100\n",
      " - 0s - loss: 0.1567 - acc: 1.0000\n",
      "Epoch 85/100\n",
      " - 0s - loss: 0.1545 - acc: 1.0000\n",
      "Epoch 86/100\n",
      " - 0s - loss: 0.1522 - acc: 1.0000\n",
      "Epoch 87/100\n",
      " - 0s - loss: 0.1501 - acc: 1.0000\n",
      "Epoch 88/100\n",
      " - 0s - loss: 0.1480 - acc: 1.0000\n",
      "Epoch 89/100\n",
      " - 0s - loss: 0.1459 - acc: 1.0000\n",
      "Epoch 90/100\n",
      " - 0s - loss: 0.1439 - acc: 1.0000\n",
      "Epoch 91/100\n",
      " - 0s - loss: 0.1419 - acc: 1.0000\n",
      "Epoch 92/100\n",
      " - 0s - loss: 0.1400 - acc: 1.0000\n",
      "Epoch 93/100\n",
      " - 0s - loss: 0.1381 - acc: 1.0000\n",
      "Epoch 94/100\n",
      " - 0s - loss: 0.1363 - acc: 1.0000\n",
      "Epoch 95/100\n",
      " - 0s - loss: 0.1345 - acc: 1.0000\n",
      "Epoch 96/100\n",
      " - 0s - loss: 0.1327 - acc: 1.0000\n",
      "Epoch 97/100\n",
      " - 0s - loss: 0.1310 - acc: 1.0000\n",
      "Epoch 98/100\n",
      " - 0s - loss: 0.1293 - acc: 1.0000\n",
      "Epoch 99/100\n",
      " - 0s - loss: 0.1276 - acc: 1.0000\n",
      "Epoch 100/100\n",
      " - 0s - loss: 0.1260 - acc: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x2319c787978>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, epochs=100, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10.7. 엘모(ELMo, Embeddings from Languages Model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ELMo는 2018년에 제안된 새로운 워드 임베딩 방법론입니다. 해석하면 '언어 모델로 하는 임베딩' 입니다.\n",
    "\n",
    "ELMo의 가장 큰 특징은 사전 훈련된 언어 모델을 사용한다는 점입입니다.\n",
    "\n",
    "다만 현재 TF2.0에서는 ELMo를 쓸 수가 없습니다. 구글 코랩에서 1.0버전으로 실습하시는 걸 권장드립니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.2.0'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10.8. 임베딩 벡터의 시각화"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "구글은 임베딩 프로젝터라는 데이터 시각화 도구를 지원합니다. 워드투벡 뿐만 아니라 글로브 등 어떤 방법으로 훈련이 되어 있더라도 시각화에는 문제가 없습니다.\n",
    "\n",
    "시각화를 위해서는 이미 모델을 학습하고 파일로 저장되어져 있어야 합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!python -m gensim.scripts.word2vec2tensor --input 모델이름 --output 모델이름"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "편의를 위해 Word2Vec 실습에 활용했던 eng_w2v를 재사용합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-07-29 14:29:45,927 - word2vec2tensor - INFO - running C:\\Users\\thinp\\anaconda3\\envs\\Wyatt37\\lib\\site-packages\\gensim\\scripts\\word2vec2tensor.py --input eng_w2v --output eng_w2v\n",
      "2020-07-29 14:29:45,927 - utils_any2vec - INFO - loading projection weights from eng_w2v\n",
      "2020-07-29 14:29:49,373 - utils_any2vec - INFO - loaded (21613, 100) matrix from eng_w2v\n",
      "2020-07-29 14:29:53,146 - word2vec2tensor - INFO - 2D tensor file saved to eng_w2v_tensor.tsv\n",
      "2020-07-29 14:29:53,146 - word2vec2tensor - INFO - Tensor metadata file saved to eng_w2v_metadata.tsv\n",
      "2020-07-29 14:29:53,149 - word2vec2tensor - INFO - finished running word2vec2tensor.py\n"
     ]
    }
   ],
   "source": [
    "!python -m gensim.scripts.word2vec2tensor --input eng_w2v --output eng_w2v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위 명령어를 수행하면 아마도 metadata.tsv와 tensor.tsv 파일이 생길 겁니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.8.2. 임베딩 프로젝터"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "링크 : https://projector.tensorflow.org/\n",
    "\n",
    "위 링크에 접속해서 진행하면 된다고 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
